java基础
过滤器和拦截器的区别
过滤器：filter，tomcat提供，在请求到达目标Servlet之前执行的，可以对任何资源进行过滤
拦截器：interceptor，动态代理，在请求到达Controller之前或者之后执行，只能对controller进行拦截

多态
多态指为不同数据类型的实体提供统一的接口。

*BIO、NIO、AIO 的区别是什么？
BIO 就是传统 IO 包，产生的最早；
NIO 是对 BIO 的改进提供了多路复用的同步非阻塞 IO，
而 AIO 是 NIO 的升级，提供了异步非阻塞 IO。

BIO是面向字节流或字符流的I/O模型，数据以流的形式经过一个一个的字节或字符传输。同步阻塞 IO
NIO则是面向缓冲区的I/O模型，数据以缓冲区的形式在通道之间传输。同步非阻塞 IO，
AIO也是面向缓冲区的，但与NIO不同的是，它在数据准备就绪后通过回调函数通知应用程序，应用程序只需等待通知即可，无需像NIO一样轮询Channel。异步非阻塞 IO




JVM
GC回收的算法有哪些
1.gc回收的算法
引用计数法：对象每引用一次+1，取消则-1，到0时没人用了就算垃圾。也会出现两个垃圾对象互相引用都不为0的情况
可达性分析算法：以GC Roots的对象作为起始点，从这些根节点开始向下搜索，不在这个引用链上的对象就是垃圾。
2.gc回收的方式
标记清除法：给垃圾对象打个标记，下次再进行清理。会留下大小不一的许多空间
标记整理法：删除之后将剩余的对象进行整理。会剩下完整的空间
复制算法：把空间分为两份，一份使用一份空着，回收时把存活的放到另一半，当前的一半进行初始化。会把使用空间减半
分代算法：对上面三个的智能选择，对于老年代和新生代的回收算法的不同。

3.堆区
在JVM中，堆区是重中之重。栈区是不会有垃圾回收的，所以经常说的垃圾回收，其实就是回收的是堆区(Heap)的数据
● 新生代：目的是回收那些生命周期短的对象，主要存放新产生的对象。。
● 老年代：存放对象生命周期较长，且内存大概是新生代的两倍。
● 永久代：指内存的永久保存区域,主要存放静态文件，如Java类，方法等。永久带对垃圾回收基本没有影响。
总结：
● 新生代中：每次垃圾收集时都有大批对象死去，只有少量存活，那就选用复制算法。只需要付出少量存活对象的复制成本就可以完成收集。
● 老年代中：因为对象存活率高、没有额外空间对他进行分配担保，就必须用标记-清除或者标记-整理。
● 永久代中：经常会内存不够用或者发生内存泄露，JDK1.8开始废弃了永久代，取而代之的是元空间（直接存在内存中可自定义大小）,主要存放类的元数据。



4.GC回收过程
 	一个对象被new出来后放伊甸园，在当伊甸园区的空间用完时后，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收(Minor GC,也叫轻GC)，将伊甸园区中的不再被其他对象所引用的对象进行销毁。然后伊甸园中的剩余对象移动到幸存0区(也叫from区)。若幸存0区也满了，再对该地区进行垃圾回收，然后移动到1区(也叫to区)。
 如果1区也满了怎么办呢？在经历了15次YGC后(每一次的gc年龄都会加一)，幸存1区也满了，那么这个时候，JVM将会把这些数据移动到养老区。如果养老区也满了，这个时候就会进行MajorGC(也称Full GC 检查FGC)。执行full GC对养老区的内存进行清理。如果养老区执行了Full GC之后，发现依然无法进行对象的保存，这个时候就会出现OOM(OutOfMemoryError)异常了。


5.oom
 OOM就是OutOfMemory就是常见的内存溢出，java.lang.OutOfMemoryError：java heap space异常。说明Java虚拟机的堆内存不够用了。主要原因有以下两种：
1：java虚拟机的堆内存设置不够，可以通过参数-Xms、-Xmx来调整
2：代码中创建了大量的大对象，并且长时间不能被垃圾回收器回收(内存地址被引用了)

什么是双亲委派机制？
当类加载器需要加载某一个.class字节码文件时，则首先会把这个任务委托给他的上级类加载器，递归这个操作，如果上级没有加载该.class文件，自己才会去加载这个.class，如果自己也也加载不到就抛出Class Not Found异常。
 总体来说，就是当一个类加载的时候首先会委托给上级类加载器去加载，当父加载器说这不是我做的事情时，则该任务又会落回到子加载器，此时只有子加载器自己完成该事情。

为什么会有这么一个机制？
避免重复加载，防止Java核心api被篡改
**逃逸分析
逃逸分析（Escape Analysis）是一种编译器技术，用于确定对象在程序执行期间是否从它们被创建的方法中“逃逸”出去（即是否由其他方法或线程引用）。如果对象没有逃逸，那么编译器可以对其进行优化，例如将其分配在堆栈上而不是堆上，以提高程序性能。

一个对象被创建后是被分配到堆内存上的，所以堆内存中会存在很多的垃圾，GC的每次回收都是在堆内存上的。

为什么栈上面没有垃圾？
方法调用的是压栈，方法结束是弹栈，所以分配到站的空间随着弹栈而被释放的。

逃逸分析能解决什么问题？
通过逃逸分析把不会逃出方法方法之外的变量给他分配栈上，从而减少而来推的占用，也就减轻了gc的压力。

**jvm的类加载过程类加载过程：
1、加载阶段；2、验证阶段；3、准备阶段，主要是将类变量在方法区进行内存分配并进行初始化；4、解析阶段；5、初始化阶段，编译器会将类文件声明的静态赋值变量和静态区域合并生成cinit方法并进行调用；6、使用阶段；7、卸载阶段。

1.加载：通过类的全限定名来读取二进制字节流，将其转换为方法区中的 Class 对象。

2.链接：分为验证、准备和解析三个阶段。
1.验证：校验类的正确性

2.准备：为给静态变量赋初始值。

3.解析：将常量池中的符号引用替换为直接引用。

3.初始化：执行类构造器 <clinit>() 方法，给静态变量赋初值，执行静态代码块。


JVM 调优
1. 堆内存设置
对于 JVM 运行时，堆内存是最常见也是最容易引起问题的地方，通常需要调整内存参数来适应应用程序的需要。其中最常见的是设置 Xms 和 Xmx 参数，分别表示 JVM 启动时堆内存的初始大小和最大大小，可以根据不同的应用场景进行调整。
另外，在进行堆内存调整时，还需要考虑新生代、老年代、堆内存分配。可以通过调整 -Xmn 参数来设置新生代大小，通过 -XX:NewRatio 和 -XX:SurvivorRatio 来控制新生代与老年代的比例和生存区大小。
1. GC 配置
GC 是 JVM 管理内存的主要手段，可以通过调整 JVM 的 GC 配置参数来控制垃圾回收的效率和时间。通常有以下几个参数：
● -XX:+UseParallelGC：表示使用并行垃圾收集器，适合在多核 CPU 上运行；
● -XX:+UseConcMarkSweepGC：表示使用 CMS 垃圾收集器，可以减少垃圾回收的停顿时间；
● -XX:+UseG1GC：表示使用 G1 垃圾收集器，可以更加灵活地管理堆内存。
1. 线程数和线程池配置
线程是 Java 应用程序的核心，线程数和线程池配置直接影响 JVM 的 CPU 和内存占用。可以通过调整线程数和线程池的参数来优化程序的性能。例如通过设置 -Xss 和 -Xmx 这些参数来控制线程栈和堆内存的大小，通过调整线程池的大小、任务队列等


集合
哪些集合是线程安全的,哪些集合是线程不安全的？
线程安全的集合：
1.  ConcurrentHashMap：线程安全的哈希表，可以在多个线程并发读写。 
2. hashtable:Java 早期提供的一个哈希表实现，它是线程安全的，不支持 null 键和值，因为它的性能不如 ConcurrentHashMap，所以很少被推荐使用；

线程不安全的集合：
1.  ArrayList：动态数组 
2.  LinkedList：双向链表 
3.  HashSet：散列表
4.  HashMap：哈希表
5.  TreeMap：有序哈希表

什么情况用线程安全，什么情况用不安全
多线程用线程安全的，单线程用不安全的

hashmap和currenthashmap的区别

1.线程安全
多个线程操作同一个容器时，可能会出现脏读的情况即读取的数据不是最新的。
例：两个线程同时操作，一个在读一个在写，可能会在没写完时读出数据
保证线程安全的方法：加锁

2.为什么加锁后就能解决
在有锁的线程执行完成前，另一个线程需要等待锁释放后才能获得锁并执行

3.三个map区别
hashmap:线程不安全，速度快，允许k/v为空值
currenthashmap:线程安全，分段锁
hashtable：早期的类，线程安全，但速度慢，且不允许k/v为空，以synchronize为锁

4.hashmap底层
hashmap的底层是数组加链表加红黑树的形式
数组和链表是为了解决hash冲突
1）.为什么用数组
是为了开辟内存空间，不全用是因为内存占用太大
2).为什么用链表
就是为了避免全用数组造成内存占用太大，通过给每个数组挂上链表可以存储很多数据
3).为什么用红黑树？
可以加快大量数据的读写速度
4).为什么不不全用红黑树？
是因为，红黑树也会占用性能，当数据量过少时反而不如数组
5).为什么不用其他树？
b+树比较适合大数据如500w以上的数据量，而完全二叉树这些会频繁回旋反而使读写速度慢。只有红黑树适合
6).hashMap的负载因子为什么是0.75？
因为能超过这个数会就会是重新元素的时间变长，低于这个数会因为提前扩容导致浪费空间，0.75刚好处在中间位置
7).equals和hashCode有什么区别？
equals:比较内容
hashCode:对象的hash值
当equals一样时，两个对象的code值一定相同
而当code值相同时，equals不一定相同

hashMap的数据结构，什么时候链表转红黑树
数组+链表/红黑树的方式来实现快速查找和插入操作。
当链表长度超过一定阈值时（默认为8），HashMap会将链表转换为红黑树结构。这是因为，当链表长度过长时，使用链表进行查找的效率会变得很低，此时采用红黑树能够更快地进行查找操作，提高HashMap的性能。

*hash冲突的原因及解决方法
使用hash结构目的就是为了提高查询效率，数组的长度有限度的，但是储存的数据是无限制的，这种情况下必然会出现hash冲突。

哈希冲突的常用解决方案有以下 4 种。

1， 开放定址法：
所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入
2， 再哈希法：
再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，….，等哈希函数，计算地址，直到无冲突。虽然不易发生聚集，但是增加了计算时间。
3， 链地址法：
链地址法的基本思想是：每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向
4， 建立公共溢出区：
这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表


b树和b+树的区别
B树和B+树都是常用的多路搜索树，主要用于外部存储器（磁盘）的数据存储和检索。两者的主要区别在于索引结构、叶子节点的结构和节点的指针数目，B+树相较于B树，有着更加稳定的索引结构和更加适用于外部存储器的特点。B树具有更好的性能表现。
 
B+树只有叶子节点存储数据，而B树的每个节点都可以存储数据。
B+树的中间节点只存储键值信息用于索引，而不像B树那样既包含键值信息又包含数据信息。
B+树的叶子节点通过指针连接成一个有序链表，方便范围查找和遍历，而B树则需要进行中序遍历得到有序结果。
B+树相比B树更加扁平、稠密，因此能够减少磁盘I/O操作次数，提高检索效率。
综上所述，B+树相对于B树来说更适合实现大规模数据存储和范围查找等应用场景。





线程
线程池的核心参数
● 1、核心线程数 corePoolSize：线程池中始终存活的线程数。
● 2、最大线程数 maximumPoolSize：线程池最大的线程数。
● 3、存活时间 keepAliveTime：除了核心线程，其余的线程都有存活时间。
● 4、时间单位 util：时、天、秒、分
● 5、拒绝策略 handler
● 6、等待队列 workQueue
● 7、创建线程池工厂 threadFactory

四个拒绝策略
● AbortPolicy：为线程池默认的拒绝策略，该策略直接抛异常处理。
● DiscardPolicy：直接抛弃不处理。
● DiscardOldestPolicy：丢弃队列中最老的任务。
● CallerRunsPolicy：将任务分配给当前调用execute方法线程来处理(同步执行)

线程池的工作原理？

当线程池中有任务需要执行时，线程池会判断如果线程数量没有超过核心数量就会新建线程进行任务执行，
如果线程池中的线程数量已经超过核心线程数，这时候任务就会被放入任务队列中排队等待执行；
如果任务队列超过最大队列数，并且线程池没有达到最大线程数，就会新建线程来执行任务；
如果超过了最大线程数，就会执行拒绝执行策略。
生成线程的方式

● 继承 Thread 类，重写 run 方法
● 实现 Runnable 接口，实现 run 方法
● 实现 Callable 接口，实现 call 方法
● 线程池

*Runnable callable 区别

返回值类型：Runnable没有返回值，而Callable有一个泛型返回值。
异常抛出：Runnable中的run()方法不能抛出异常，而Callable的call()方法可以抛出异常。

总之，如果需要执行的任务不需要返回值或抛出异常，可以使用Runnable。如果需要返回值或抛出异常，则应该使用Callable。

mysql

*mysql底层原理
MySQL 是一种关系型数据库管理系统，其底层原理涉及多个方面：
存储引擎：MySQL 支持多种不同的存储引擎，每种引擎都有自己的数据存储方式和查询处理方式。其中 InnoDB 是 MySQL 默认的事务性存储引擎，采用了行级锁和 MVCC 策略来提高并发性能。

数据结构：mysql底层使用b+树来组织数据。 b+树是一棵稳定的平衡树，由一个根节点、中间节点和叶子节点组成。 根节点和中间节点都用于索引，而叶节点包含全部键值和每个键的数据值。

SQL 查询处理：MySQL 通过解析 SQL 查询语句，将其转换为执行计划，然后执行该计划以获取查询结果。执行计划通常由多个操作符组成，包括选择、投影、连接和聚合等操作。

索引：MySQL 使用 B+ 树索引来加速数据访问。索引可以大大减少查询时需要扫描的数据量，提高查询效率。同时，索引也需要消耗额外的空间和维护成本。

优化器：MySQL 优化器负责对查询执行计划进行优化，以提高查询性能。优化器会考虑多种因素，如索引使用情况、表大小、统计信息等，来选择最优的执行计划。

缓存：MySQL 提供了多种缓存机制来加速数据访问。其中包括查询缓存、表缓存和 InnoDB 缓存等。这些缓存可以显著减少
为什么用索引就快了？底层怎么实现的？
索引就是把无序的数据变成有序的查询，索引的本质是一种排好序的数据结构。按照特定的方式对数据进行排序和组织的，而不是按照数据在表中的物理存储顺序。执行查询时就可以直接跳过不符合条件的行，从而减少了查询的数据量，提高了查询效率。
因为它是树结构，树的特点是左小右大，根节点左边的数据要比根节点小，右边的要比根节点大，它是一个这么一个结构，在插入的时候就已经维护了平衡二叉树。树节点在查找的时候，先从根节点开始，如果比根节点大就去右子树去找，否则去左子树去找，以此类推我们只需要从一半的数据中寻找自己想要的值。
底层实现的方式：
B+tree：B-tree是一种自平衡的树形数据结构，可以高效地支持范围查询；
Hash：Hash则是一种基于哈希函数的数据结构，可以快速地查找具有给定关键字的记录。 


mysql如何去除冗余数据
distinct：去除重复数据
set：转换为set类型，利用set不可重复的特性
group by
sql优化
四点出发：硬件、储存系统（SQL,NOSQL）、储存结构（储存引擎）、具体实现（sql语句）

储存结构（储存引擎）
innodb引擎：默认使用，它支持事务和行级锁定，并提供更好的数据完整性和可靠性。
myisam引擎：myisam是另一个mysql储存引擎，它不支持特定的事务或行级锁定功能。读多写少时
选择合适的储存系统
分库分表
硬件的优化
1、 更大的内存，减少磁盘IO
2、采用多核CPU，可以提高MySQL的执行速度
3、采用性能更好的固态硬盘

1. 使用索引：索引可以大大提高 SQL 查询的效率，加快数据的访问速度。因此，在选择主键和其他需要频繁查询的列时，建议增加索引。
2. 优化查询语句：在编写 SQL 查询语句时，要注意使用合适的查询条件和运算符，避免使用大量的子查询和联合查询，及时删除不必要的字段和表，简化查询条件。
3. 避免数据太大：大数据量会极大地影响 SQL 的查询效率。因此，在设计数据库表结构时，要避免数据冗余和不必要的字段，保持数据的精简性。
5. 分库分表：根据业务需求，可以将一个大数据表分成多个小表，提高查询速度。。

1、尽量避免在字段开头模糊查询 
2、尽量避免进行null值的判断
3、尽量避免在where条件中等号的左侧进行表达式
4、让order by 走索引
5、避免出现select *
6、多表关联查询时，小表在前，大表在后
7、多次插入改为批量插入

*索引
索引是一种特殊的的文件，包含着对数据所有记录的指针，能够把无序的数据以有序的形式排列好，查询时可以忽略无关数据量以此来增加查询速度。
索引优点：可以大大加快数据的检索速度，这也是创建索引的最主要的原因。
索引的缺点：
时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率
空间方面：索引需要占物理空间

索引类型
主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。
唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。
单值索引: 给表中的某一个字段添加索引，
组合索引: 给表中的多个字段添加索引，主要针对多条件查询
全文索引：就和ES有点像，根据关键字去匹配

*MySQL主键和唯一索引的区别？
1、主键是一种约束，唯一索引是一种索引，两者在本质上是不同的。
2、主键创建后一定包含一个唯一索引，唯一索引并不一定就是主键。
3、唯一性索引列允许空值，而主键列不允许为空值。
4、一个表最多只能创建一个主键，但可以创建多个唯一索引。
5、主键更适合那些不容易更改的唯一标识，如自动递增列、身份证号等。


*聚簇索引
将数据存储与索引放到了一块，索引结构的叶子节点保存了行数据的B+树称为聚簇索引。通过一次索引就能找到叶子节点的行数据聚簇索引，一般都是主键索引。而非聚簇索引需要二次查询
InnoDB使用的是聚簇索引，将主键按大小排序规则建立的B+树中，若使用"where id = 14"这样的条件查找主键，则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据。
聚簇索引与非聚簇索引的区别
聚簇索引：表中行的物理顺序与键值的逻辑顺序相同。
非聚簇索引：聚集索引和非聚集索引的根本区别是表记录的排列顺序和索引的排列顺序是否一致。

联合索引
MySQL联合索引遵循最左前缀匹配规则，即从联合索引的最左列开始向右匹配，直到遇到匹配终止条件。例如联合索引(c1, c2, c3),c1会走索引，c1 c2 会走索引，c1c2c3 会走索引，c1 c3会走索引，c2 c3不会走索引。

前缀索引
前缀索引是选择字符列的前n个字符作为索引，这样可以减少索引空间大小，降低重复的索引值，从而提高索引效率。

什么时候应该加索引，需要注意什么
经常需要查询的列，数据量较大的表
注意添加索引会增加数据库的存储空间，同时也会增加插入、更新和删除操作的时间。需要根据情况添加索引

MySQL是如何保证唯一性索引的唯一性的？
MySQL通过在创建索引时向B+树节点中插入一个唯一键值，来保证唯一性索引的唯一性。

如何判断sql语句有没有走索引？
使用 EXPLAIN 命令：在sql语句前加explain关键字
监控查询日志和慢查询日志

执行explain查询后，最常看的参数是哪些
1.type：表示访问表的方式，是一个很重要的参数。如果是使用了好的索引，type就会显示为const或者ref，否则可能会显示为全表扫描（ALL），这会大大影响查询性能。
2.key：表示实际被使用的索引，如果该值为空，则意味着没有使用合适的索引进行优化查询。
3.rows：表示MySQL估计需要扫描的行数，该值对于查询优化非常重要。如果rows过大，则需要考虑优化查询语句或者增加适当的索引。
4.Extra：表示额外的信息，如Using filesort、Using temporary等。这些信息结合上面的参数一起分析可以帮助我们定位SQL查询的瓶颈。
5.possible_keys：表示所有可能被使用的索引，但不一定全部都会被使用。该参数用于判断是否存在可用的索引，如果该值为空，则说明需要优化查询语句或者添加适当的索引。

*回表
回表，顾名思义就是回到表中，也就是先通过普通索引扫描出数据所在的行，再通过行主键ID 取出索引中未包含的数据。
简单来说回表就是 MySQL 要先查询到主键索引，然后再用主键索引定位到数据
mysql主键和索引的区别
主键本身就是索引，它是聚簇索引，它的叶子节点上面保存的是行数据。
二级索引/非聚簇索引：叶子节点上只有主键和索引那一列，需要其他的属性则需要回表。

数据库中一张表的数据达到2亿，查询慢，如何优化？
1、优化sql语句(执行计划看下这条sql是否是最优的)
2、添加索引
3、数据的极度压缩，定时任务去跑数据，然后把结果插入到一张表中。
4、分库分表

*幂等性,保证幂等性的措施有哪些?
概念：用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。
   同一个接口多次调用不会对数据造成影响。

项目中那些场景需要解决幂等性的？
下单的接口，用户频繁点击或者因为一些网络的原因，导致本该发送一次请求的，却发送了多次，而此时接口并没有考虑到这种情况，造成的数据库有多个订单成功了。
方法：发送验证码的接口，点完一次后需要把按钮设置成警用状态。

保证幂等性的措施和方法有以下几种：
1. 唯一标识：在请求中添加唯一标识，如UUID等，每次请求都带上该标识，服务器在接受到请求后先检查该标识是否已经处理过，如果已经处理过，则直接返回结果，否则处理请求并将该标识标记为已处理。
2. 乐观锁：在数据库中使用乐观锁机制，每次更新数据时，检查数据版本号是否一致，如果一致，则进行更新操作，否则返回错误信息。
3. 幂等性Token：在每次请求中添加一个幂等性Token，服务器在接收到请求时，先从Token中获取请求的ID，再根据ID判断该请求是否已经处理过，如果已经处理过，则直接返回结果，否则处理请求并将该请求ID标记为已处理。
4. 重复请求过滤：在请求到达服务器之前，使用请求过滤器或者网关进行判断，如果检测到该请求已经被处理，则直接返回结果，否则将请求转发到后端服务器进行处理。
总之，保证幂等性是分布式系统中必须要考虑的问题，可以通过唯一标识、乐观锁、幂等性Token、重复请求过滤等措施来实现。不同的措施适用于不同的场景和需求，需要根据具体情况进行选择和优化。

解决接口幂等问题
"一锁、二判、三更新"

一锁：第一步，先加锁。可以加分布式锁、或者悲观锁都可以。但是一定要是一个互斥锁!
二判：第二步,进行幂等性判断。可以基于状态机、流水表、唯一性索引等等进行重复操作的判断。
三更新：第三步,进行数据的更新,将数据进行持久化。


char和varchar的区别？
1.存储方式：char会以固定长度的方式存储，而varchar则以可变长度的方式存储。
2.存储空间：由于char以固定长度的方式存储，因此对于相同数量的字符，char需要更多的存储空间。而varchar则只使用必要的存储空间，可以节省存储空间。
3.比较效率：由于char以固定长度的方式存储，因此比较时速度更快。而varchar则需要在比较之前计算字符串的长度，可能会稍微慢一些。

*主键回填
主键回填是指在插入一条记录时，数据库自动生成主键值，并将其回填到该记录中的过程。通常用于使用自增长方式生成主键的情况。
常见的实现方式：

1. 使用 JDBC 的 getGeneratedKeys() 方法获取自动生成的主键值，并将其回填到 Java 对象的成员变量中。

2. 使用 MyBatis 框架提供的 useGeneratedKeys 和 keyProperty 属性来实现主键回填。useGeneratedKeys 属性表示是否使用自动生成的主键，keyProperty 属性表示将自动生成的主键回填到哪个成员变量中。

3.mybatis-plus  @TableId注解来实现
InnoDB和MyISAM有什么区别？


InnoDB和MyISAM是MySQL数据库的两种不同的存储引擎。

1.事务支持：InnoDB支持事务处理，而MyISAM不支持。
2.行级锁定：InnoDB支持行级锁定，可以避免多个用户同时修改同一行数据时产生的冲突问题；而MyISAM只支持表级锁定，不能避免以上问题。
3.外键支持：InnoDB支持外键约束，可以保证数据的一致性；而MyISAM不支持外键。
4.性能表现：对于大量并发查询的环境，InnoDB的性能比MyISAM更好，但对于大量写入操作的环境，MyISAM的性能可能更好。

总之，如果需要高并发、高可用、高安全、高可靠的数据库集群服务，建议使用InnoDB存储引擎；
而如果需要对表进行大量的INSERT和SELECT操作，并且不依赖事务的完整性和一致性，可以使用MyISAM存储引擎

MySQL中的行锁和表锁
MySQL中，InnoDB引擎是支持行锁的，而MyISAM引擎则只支持表锁。具体如下：
行级锁
行级锁是Mysq|中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。

他的特点是开销大，加锁慢，优点是锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
表级锁
表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单,资源消耗较少,被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。

他的优点是开销小，加锁快，锁缺点是锁定粒度大,发生锁冲突的概率最高,并发度最低。

意向锁
当多个事务想要访问一个共享资源的时候，如果每个事务都直接请求获取锁，那么就可能会导致互相阻塞，甚至导致死锁。
当一个事务请求获取一个行级锁或表级锁时，MySQL会自动获取相应的表的意向锁。
这样，其他事务请求获取锁时，就可以先基于这个意向锁来发现是否有人加过锁，并根据该锁的类型来判断自己是否可以获取锁。这样可以在不阻塞其他事务的情况下，为当前事务锁定资源。

意向锁有两种类型:
意向共享锁：用于表明事务希望获取一个共享锁(读锁)
意向排他锁，用于表明事务希望获取一个排他锁(写锁)

同时意向锁是一个表级锁


关系型数据库和非关系型数据库主要有哪些区别?
1、关系型数据库以表的形式进行存储数据，非关系型数据库以Key-value的形式存储数据。
2、关系型数据库需要保证事务的ACID， 非关系型数据库中的事务一般无法回滚。(也有部分数据库可
以回滚,如mangodb在集群模式下)
3、关系型数据库可以通过一张表中的任意字段进行查询，非关系型数据库需要通过key进行查询
4、关系型数据库通常基于硬盘存储，非关系型数据库基于内存存储。
5、关系型数据库支持各种范围查询、公式计算等,非关系型数据库不一定支持。


数据库三大范式
第一范式（1NF）：所有关系型数据库都符合第一范式要求，即每个属性都是原子性的，不可再分。

第二范式（2NF）：在满足第一范式的基础上，非主键属性必须完全依赖于关系中的每一个候选键。

第三范式（3NF）：在满足第二范式的基础上，任何非主属性不依赖于其他非主属性。

反范式是指在数据库设计中，增加冗余数据以提高查询性能或简化应用程序的操作的过程。通常这涉及将重复信息存储在多个表中，从而避免频繁地进行连接操作。反范式可以提高读取数据性能，但会增加数据冗余和更新操作的复杂度。

事务的隔离级别，脏读、幻读、不可重复读

四种隔离级别 ：      未提交读                                    提交读               可重复读      序列化
对应的出错：幻读、脏读、不可重复读          幻读、不可重复读           幻读

脏读：读到了一个事务没有提交的事务。

不可重复读：一个事务多次读取结果不同。

幻读：一个事务在更新，一个事务在做插入。

脏读:读到了其他事务还没有提交的数据。
不可重复读:对某数据进行读取过程中,有其他事务对数据进行了修改(UPDATE、 DELETE)， 导致第二次读取的结果不同。
幻读:事务在做范围查询过程中，有另外一个事务对范围内新增了记录(INSERT)，导致范围查询的结果条数不一致。


隔离级别的相关命令
查看当前会话事务隔离级别 select @transaction_isolation
查看全局事务的隔离级别 select @global.transaction_isolation;


MVCC
多版本并发控制，MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。

当前读和快照读
快照读就是读取的是快照数据,即快照生成的那一刻的数据
当前读当前读就是读取最新数据，读取时会保证其他并发事务不能修改当前记录，会对读取的记录进行加锁是悲观锁的实现

MVCC 就是为了实现读-写冲突不加锁，而这个读指的就是快照读
MVCC 就是不满意只让数据库采用悲观锁这样性能不佳的形式去解决读-写冲突问题，而提出的解决方案

覆盖索引
指一个查询语句的执行只要从索引中就能够取得，不必从数据表中读取。 也可以称之为实现了索弓|覆盖。
索引下推
索引下推（Index Pushdown）是一种优化数据库查询性能的技术，它将部分查询操作从数据库系统中的执行引擎下推到存储引擎中执行。这样可以减少数据的读取和传输，提高查询效率。具体来说，索引下推可以将一些过滤、投影和排序等查询操作下推到存储引擎中执行，以减少需要处理的数据量，从而提高查询性能。








ssm

*Spring 的注解
Spring 的注解是通过 Java 反射机制实现的。在运行时，Spring 使用反射来扫描标记了注解的类、方法和属性，并根据这些注解执行相应的操作。


springmvc的执行流程发送的请求怎么确定对应的controller
通过映射从 IoC 容器中获取对应的 Controller 对象

Mybatis的缓存
 缓存：可以更快和临时保存数据
1.为什么要有缓存？
内存可以理解为缓存，内存读写速度很快，可以把经常查询的数据保存到缓存中，以提高读写速度

2.底层怎么实现？
缓存底层都是map，key-value

3.一致性问题
同一个数据保存在两个地方，无法保证缓存以及数据库里的原子性，所以缓存一致性本质上是无法解决的。
如直播在线人数，每个用户实时观看的在线人数都不同，但结束时的数据时一样的
项目的数据是允许出现一致性的问题，如果允许就保证最终一致性。如果不允许出现一致性(强一致性)的问题就不要使用缓存了，直接查询数据库。

Mybatis的一级缓存是指Session缓存。一级缓存的作用域默认是一个SqlSession。Mybatis默认开启一级缓存。
Mybatis的二级缓存是指mapper映射文件。二级缓存的作用域是同一个SQLSessionFactory下的mapper映射文件内容，多个SqlSession共享。


mybatis中#{}和${}的区别

#{}：表示使用预编译语句，将参数以占位符的形式传递给 JDBC 驱动程序，驱动程序会将参数进行转义处理有效的防止 SQL 注入，保证程序的运行安全。
${}：表示将参数以字符串的形式直接拼接在 SQL 语句中，存在 SQL 注入攻击风险。如果传入的参数是变量或表名，也需要使用 ${} 进行拼接。

*mybatis的实现流程

加载配置文件：在启动阶段，MyBatis会加载一个XML格式的配置文件，其中包含了数据源、SQL映射文件等信息。
创建SqlSessionFactory：通过读取配置文件中的信息，创建SqlSessionFactory对象，它是MyBatis的核心对象，用于创建SqlSession对象。
创建SqlSession：SqlSession是MyBatis的连接对象，它提供了与数据库交互的方法，比如执行SQL语句、获取映射器等。通常情况下，每个线程都应该拥有自己的SqlSession对象。
获取Mapper：通过SqlSession对象获取Mapper接口的实例，Mapper接口中定义了对数据库表的操作方法。
执行SQL语句：调用Mapper接口中的方法，MyBatis会将方法名和参数解析成一条SQL语句，并执行该语句。
处理结果集：执行完SQL语句后，MyBatis会将结果集转换成Java对象或基本类型返回给调用者。
关闭SqlSession：使用完SqlSession之后，需要关闭它以释放资源。

核心的Bean
Configuration：XML中的内容解析后全部放到这个类中
SqlSessionFactory：创建SqlSession
SqlSession：它和数据库的会话，否则创建Mapper代理是，事务
MapperProxy：使用JDK给Mapper创建代理
MapperMethod：对调用方法的一个封装(SQL语句，方法的信息)
MappedStatement：对Mapper文件的封装
Executor：执行SQL语句
RowBounds：分页的工具


分页插件如何实现 ：拦截sql语句，通过预编译处

动态sql如何写：用if和where 用#

方法和sql如何绑定 先namespace 和id

如何修改Spring创建的Bean是多列的
把scope设置为prototype时，spring设置的是多实例方案。
<bean id="user" scope="prototype"><bean>









springboot
解决了什么问题？
简化了依赖的配置，开发人员只需要3步就能完成。
1.导入依赖
2.yml配置
3.注入xxxtemplate开始使用
**SpringBoot自动装配机制

1.注解
@SpringBootApplication=@SpringBootConfiguration+@EnableAutoConfiguration+@ComponentScan
@SpringBootConfiguration：配置类
@ComponentScan：扫描类，默认会扫描当前类下面的所有的子包
@EnableAutoConfiguration：实现自动装配

2.如何实现
SpringBoot启动的时候加载主配置类@SpringBootApplication，开启了自动配置功能@EnableAutoConfiguration。它会根据classpath下的jar包和项目中定义的Bean来自动配置Spring应用，并将这些Bean注入到IoC容器中。
自动配置机制会根据一系列的条件来判断是否需要自动装配某个Bean实例Spring Boot会自动检测项目中的依赖关系，并根据依赖关系自动装配相应的Bean实例。
如果项目中存在多个实现类，Spring Boot会根据@Primary注解或@Qualifier注解进行选择和装配。

总之，SpringBoot自动装配机制是通过IoC容器和自动配置组件相结合的方式来实现的，它可以大大简化开发者的开发工作，提高应用程序的可维护性和稳定性。 

3.Spring的导入
静态导入：导入的内容是不变的。
通过@Ipmort标签可以导入其他类中的方法到当前类中。
动态导入：导入的内容是动态的，不同情况导入的内容是不一样的。
实现ImportSelector借口，可以导入多个类的方法返回数组








springcloud
ribbon和feign区别
ribbon和feign都是SpringCloud提供的组件。
Ribbon：负载均衡
feign：远程调用，服务之间的通讯

Ribbon是一个客户端负载均衡器，主要用于在客户端发起请求时，根据一定的负载均衡策略，从多个服务提供方中选择一个进行处理；
Feign是一个基于Ribbon实现的声明式HTTP客户端，它可以让开发者更加方便地定义和调用RESTful服务。

负载均衡
1.什么是负载均衡
负载均衡其意思就是分摊到多个操作单元上进行执行。如果只有一台服务器会出现故障和性能问题，所以需要集群生成多个服务器，通过负载均衡分摊到多个服务器

**2.负载均衡的算法/策略
轮训：每个服务器轮一次
随机：随机选择
ip-hash：根据定值来计算hash，然后取一个
权重：由用户设置，权重越高优先级越高
响应时间：时间越少速度越快，优先级越高

3.客户端负载均衡和服务端负载均衡
服务端：反向代理，由服务端选择哪个服务器	nginx
客户端：由客户端选择，由nacos拉取多个服务，客户端来选择  ribbon

*4.nginx
Nginx是一个高性能的HTTP和反向代理web服务器,占有内存小，并发能力强，作用：1、作为Web服务器；2、作为负载均衡服务器；3、Nginx还能实现动静分离，简单来说就是把动态跟静态请求分开，可以理解成使用Nginx处理静态页面，Tomcat处理动态页面。

远程调用技术有哪些
远程调用：两个不同服务器，其中一个需要调用另一个的方法。本质就是发送请求
本地调用：在同一个服务器中的方法进行调用。如Controller调用service，service调用mapper

远程调用有哪些：
Java.net.URL -->Java网络编程包--》发送一个请求
Spring的RetTemplate可以发送请求
SpringCloud的OpenFein可以发送http请求
Springalibaba的Dubbo也是远程调用技术

异步调用远程接口如何获取返回值
提供者把消息发送给队列，消费者监听队列，这样就做到了异步调用。
消费者再把消息发给一个队列，提供者监听这个队列，这样就做到了获取返回值。
像这种异步调用都要设置回调函数。


springcloud
1.springcloud的组件有哪些
注册中心：Eureka
远程调用：feign
负载均衡：ribbon
配置：config
网关：zuul
熔断降级：hystrix
服务日志：sluth


2.SpringCloud是干什么的？
Spring提供实现微服务开发的一套解决方案。
Spring：核心组件，主要提供了IOC和AOP。
SpringMVC：spring对前端进行交互，专门用来开发controller接口
SpringBoot：它是快速构建的Spring应用程序，自动转配，约定大于配置
SpringCloud：供实现微服务开发的一套解决方案。

3.SpringCloudAlibaba组件
注册中心：nacos
远程调用：dubbo
熔断降级：sentinel
分布式事务：seata
网关：gateway
对象储存(文件服务器)：OSS

4、该如何选择
任何一个微服务项目多有以下组件
注册中心：nacos/eureka
配置中心：nacos/eureka
服务网关：gatway/zuul
熔断降级：sentinle/hystix
远程调用：feign/dubbo
负载均衡：ribbon

Nacos
Nacos是一个基于DNS和HTTP的服务注册和发现中心，它提供了服务注册、配置管理和服务发现功能，可以实现微服务的服务注册、服务发现和部署等功能，是一个非常重要的微服务基础设施。
Nacos的原理主要包括以下几个方面：
1. 注册中心：Nacos通过一个注册中心来管理各个微服务的注册和发现。每当一个微服务启动时，它会向注册中心注册自己的信息，包括服务名、IP地址、端口号和健康状态等。注册中心将这些信息存储起来，并对客户端提供服务注册和发现的接口。
2. 服务发现：当一个客户端需要调用某个服务时，它可以通过Nacos提供的服务发现接口来获取该服务的地址和端口号。Nacos的服务发现采用了DNS协议和HTTP协议，即当客户端需要调用某个服务时，它可以通过DNS协议从Nacos服务器获取该服务的IP地址和端口号，然后通过HTTP协议与该服务进行通信。
3. 配置管理：Nacos还提供了配置管理的功能，可以将各个微服务的配置信息统一管理。每个微服务可以通过Nacos提供的配置管理接口，将自己的配置信息注册到Nacos服务器，并在需要更新配置时，可以通过该接口更新自己的配置信息。Nacos将这些配置信息存储起来，并对客户端提供各种配置查询和更新接口。
4. 集群支持：Nacos还支持集群部署，可以将多个Nacos节点组成一个集群，共同管理各个微服务的注册和发现，从而提高了系统的可用性和稳定性。
总之，Nacos的原理主要是通过注册中心管理各个微服务的注册和发现，通过服务发现、配置管理等功能提供统一的服务入口和配置管理平台，使得微服务系统更加容易管理和运维。
谈谈你对微服务和分布式的区别
微服务：软件架构的思想，更注重在业务划分上，可以采用微服务架构来实现，比如：拆分成多个服务，每个服务都要独立的部署。

分布式：分布式部署,注重于数据和计算任务的分散化，比如：在一个服务器中可以部署多个服务，也可以部署多个中心(地区)。
谈谈你对CAP和Base的理解，它们都应用在哪里
CAP
一致性(Consistency)、可用性(Availability)、分区容错性(Partition tolerance)
一致性：任何时候读取节点返回的数据都是一致的
可用性：每次访问时返回数据就行，对一致性要求不高

Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式；Zookeeper（保证CP）

*Base理论
BA：基本可用，S：软状态，E：最终一致性。

● 基于CAP理论演化而来的，是对CAP定理中一致性和可用性的一个权衡结果。
● 核心思想：我们无法做到强一致性时，但是每一个应用都可以根据自身的业务特点，采用一些适当的方式来权衡，最终达到一致性。
● BA：分布式系统中因为一个原因，导致出现了一些问题，允许损失掉部分服务的可用性，保证我核心功能的高可用。
● S：允许系统之间存在一个中间状态，并不会影响正常的去使用整个系统，允许数据的同步存在延迟。
● E：系统中所有的数据副本经过一定时间后，最终能够达到一致的状态，不需要保证系统数据强一致性。


设计接口的时候如果要考虑到断网怎么解决？
1. 缓存中间数据
设计接口的时候，可以考虑将某些数据缓存在本地，这样即使网络不畅或者断网，应用程序依然能够正常加载之前缓存的数据并进行相应操作，提高应用程序的可用性。

2. 合理设置超时时间
在设计接口时，考虑使用超时时间来解决网络连接不稳定的情况。设置合理的超时时间，可以避免应用程序在等待网络响应时无限期的挂起。
什么是柔性事务？什么是刚性事务？
1. 刚性事务
所有的操作都必须要满足 ACID（原子性、一致性、隔离性、持久性），能够保证数据的一致性和完整性。刚性事务的实现通常借助数据库事务技术，可以保证在单机或单节点上的事务执行一致性。

2. 柔性事务
柔性事务又称为最终一致性事务，是指在分布式系统中的一组任务或操作，执行需要考虑到数据一致性，但这组任务或操作中的所有子任务不一定达成一致，最终能达成一定的一致性即可。柔性事务不强调准确和强一致性，而倾向于追求业务需求和应用场景下的柔性一致性。柔性事务的实现依赖于消息队列、事件驱动架构、定时任务等方式实现。



Redis
reids数据类型和应用场景
redis在一般应用：
String：缓存，限流，计数器，分布式锁
hash：存储用户信息，用户主页访问量，组合查询
list：微博关注人时间轴列表，简单队列
set：赞、踩、标签、好友关系
zset：积分榜 
应用场景
1、缓存服务器
2、分布式锁
3、超时机制
4、计数器
5、验证码
6、排列

数据库和redis如何保证数据的一致性
1.给缓存设置过期时间
2.写完DB后，删除redis （延时双删）
3.当前服务修改完DB后，写消息到MQ，另一个服务拿到消息后做修改。

给缓存设置过期时间，是保证最终一致性的终极解决方案。
这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。
延时双删的策略，就是在更新数据库之前先删除缓存，然后对数据库进行写入操作，数据库更新完成之后再次进行删除缓存的操作，目的是删除读请求可能造成的缓存脏数据

redis的自动淘汰机制
1. 惰性删除：当访问key时，再判断key是否过期。过期则删除并返回null。
2. 定期删除：周期性地随机测试一批设置了过期时间的key并处理。过期则删除。
Redis默认同时开启定期删除和惰性删除两种过期策略。

Redis的内存淘汰策略有哪些
淘汰算法
a)Random：随机淘汰
b)LRU：是淘汰最长时间没有被使用的
c)LFU：是淘汰一段时间内，使用次数最少的。
e)ttl：根据过期时间
f)noeviction:默认算法，内存满的时候只支持读，不支持写0、Redis在内存不足的时候，会进行key的淘汰

LRU:如果一个数据在最近一段时间没有被访问到, 那么可以认为在将来它被访问的可能性也很小。
因此，当空间满时，最久没有访问的数据最先被淘汰。


LFU:如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能
性也很小。因此，当空间满时，最小频率访问的数据最先被淘汰。LFU的每个数据块都有一个引用计数,所有数据块按照引用计数排序，具有相引|用计数的数据块则按照时间排序。



**redis命令执行流程
当用户输入一条命令之后，客户端会以 socket 的方式把数据转换成 Redis 协议，并发送至服务器端，服务器端在接受到数据之后，将协议转换为执行命令，经过各种验证以保证命令能够正确并安全的执行，但验证处理完之后，会调用具体的方法执行此条命令，执行完成之后会进行相关的统计和记录，然后再把执行结果返回给客户端。

redis是单线程还是多线程
单线程：网络和数据读写
多线程：过期时间、持久化、数据同步

*Redis为什么快
Redis 之所以如此快，主要有以下几个方面的原因:
1、基于内存: Redis是一种基于内存的数据库，数据存储在内存中，数据的读写速度非常快，因为内存访问速度比硬盘访问速度快得多。
2、单线程模型: Redis使用单线程模型，这意味着它的所有操作都是在一个线程内完成的，不需要进行线程切换和上下文切换。这大大提高了Redis 的运行效率和响应速度。
3、异步非阻塞的网络I/O模型: Redis 采用异步非阻塞的网络I/O模型，能够处理大量的客户端连接请求，同时保持较低的系统负载。Redis 通过I/О多路复用技术，实现了单个线程同时处理多个客户端连接的能力，从而提高了Redis的并发性能。
4、高效的数据结构: Redis提供了多种高效的数据结构，如哈希表、有序集合、列表等，这些数据结构都被实现得非常高效，能够在O(1)的时间复杂度内完成数据读写操作，这也是Redis 能够快速处理数据请求的重要因素之一。
5、多线程的引入:在Redis 6.0中，为了进一步提升IO的性能，引入了多线程的机制。采用多线程，使得网络处理的请求并发进行，就可以大大的提升性能。多线程除了可以减少由于网络Io等待造成的影响，还可以充分利用CPU的多核优势。

同步、异步、阻塞、非阻塞

同步与异步描述的是被调用者的。
譬如A调用B。
如果是同步，B在接到A的调用后，会立即执行要做的事。A的本次调用可以得到结果。
如果是异步，B在接到A的调用后，不保证会立即执行要做的事，但是保证会去做，B在做好了之后会通知A。A的本次调用得不到结果，但是B执行完之后会通知A。

阻塞与非阻塞描述的是调用者的。
A调用B。
如果是阻塞，A在发出调用后，要一直等待，等着B返回结果。
如果是非阻塞，A在发出调用后，不需要等待，可以去做自己的事情。
同步不一定阻塞，异步也不一定非阻塞。没有必然关系。

IO多路复用
IO多路复用是指同时监控多个IO操作，可以在同一个线程中实现对多个IO操作的处理，避免了不必要的线程切换和资源浪费,提高了程序执行效率和性能。

Redis的虚拟内存机制是什么？
Redis提供了一种称为虚拟内存的机制，用于将部分不经常使用的数据存储到磁盘上，从而避免Redis进程占用过多的内存。
当Redis使用的内存超过了指定的阈值时，虚拟内存机制将自动将一些键值对转移到磁盘上，以释放一部分内存。当需要访问时虚拟内存机制将自动把数据读取到内存中。

Redis的持久化机制
RDB（快照）：保存一瞬间的内存结构，持久化到磁盘
快照文件小、恢复速度快，适合做数据恢复。
定期更新可能会丢数据
AOF（只读）：只保留写命令到磁盘中
可以实现更高的数据可靠性、支持更细粒度的数据恢复，适合做数据存档和数据备份。
文件大占用空间更多,每次写操作都需要写磁盘导致负载较高
特性	RDB	AOF
数据可靠性	可能会失最后-次快照之后的数	保证最后一 次写操作之前的数据不会丢失
性能	读写性能较高，适合做数据恢复	写性能较高,适给做数据存档
储存空间占用	快照文件较小，占用空间较少	AOF文件较大，占用空间较多
恢复时间	从快照文件中恢复数据较快	从AOF文件中恢复数据较慢
Redis 的事务机制是怎样的?
Redis中是支持事务的,他的事务主要目的是保证多个命令执行的原子性,即要在一个原子操作中执行, 不会被打断。.
需要注意的是，Redis的事务是不支持回滚的，如果事务执行过程中发生错误，Redis 会继续执行剩余的命令而不是回滚整个事务。


什么是热Key问题，如何解决热key问题
一个key在1秒钟之内被访问次数达到上千次,就可以认为是热key了。

对于热key的处理，主要在于事前预测和事中解决。
事前预测就是根据一些根据经验, 提前的识别出可能成为热key的Key,比如大促秒杀活动等
事中解决方面，主要可以考虑，热点key拆分、多级缓存、热key备份、限流等方案来解决。
多级缓存：通过缓存的方式尽量减少系统交互，使得用户请求可以提前返回。
热点key拆分：将一个热key拆分成多个key,在每一个Key后面加一 个后缀名,然后把这些key分散到多个阶段中。
热key备份：部署多 个缓存服务器，如Redis同时部署多个服务器集群

Redis的数据分片
Redis的数据分片(sharding) 是一种将一 个Redis数据集分割成多个部分，分别存储在不同的Redis节点上的技术。
在Redis的Cluster集群模式中，使用哈希槽(hash slot)的方式来进行数据分片，将整个数据集划分为16384个槽，每个槽都有一个编号(0~16383) ，集群的每个节点可以负责多个hash槽，客户端访问数据时,先根据key计算出对应的槽编号,然后根据槽编号找到负责该槽的节点,向该节点跋送请求。
16384这个数量在实践中被证明是一个比较合适的选择, 能够在保证负载均衡的同时，减少数据迁移的开销

什么是大Key问题，如何解决？
Big Key是Redis中存储了大量数据的Key,不要误以为big key只是表示Key的值很大,他还包括这个Key对应的value占用空间很多的情况，

危害:影响性能、占用内存、内存空间不均匀、影响Redis备份和恢复、搜索困难、迁移困难、过期执行耗时

通常情况下，建议一个key的value大小最好不要超过1MB, 一个集合(Set)或有集合(ZSet)中元素的数量最好不要超过10000个，超过这些数量就可能会影响Redis的性能。

Redis中的big key可以识别的程序是"redis- cli"

解决Big Key的问题: 
1、有选择地删除Big Key
2、通过合理的设置缓存TTL,
3、拆分，把big的key拆分开:
a、在业务代码中,将一个big key有意的进行拆分,比如根据日期或者户尾号之类的进行拆分。使用小键替代大键可以有效减小存储空间，从而避免影响系统性能
b、使用master/slave复制或者集群，以将大key分散到不同服务器上,以加快响应速度。
4、部分迁移:存放在单独的数据库中,从而实现对大key的部分迁移


*Redis如何实现延时消息
Redis可以通过Zset（有序集合）来实现延迟消息的功能。


1.在有序集合中添加一个元素，该元素的score为消息的过期时间戳，成员为消息ID。
2.使用Redis提供的定时器功能，定时扫描有序集合中score值小于当前时间戳的元素，并将对应的消息取出来进行处理。
3.处理完消息后，从有序集合中删除对应的元素。
这种方式可以确保消息会在指定的时间点被处理，同时能够避免轮询查询的性能问题。

Redis 的主从复制
是一种数据同步机制，它通过将一个 Redis 节点（主节点）的数据复制到多个其他节点（从节点）上，以实现数据的备份、读写分离和负载均衡等功能。主节点负责接收写请求和更新自身数据，而从节点只能接收读请求并复制主节点的数据。

Redis 主从复制的过程如下：

1、从节点连接主节点并发送 SYNC 命令请求复制数据。
2、主节点接收到 SYNC 请求后开始执行 BGSAVE 命令生成 RDB 快照文件，并使用缓冲区保存期间的写操作命令。
3、主节点在生成快照文件后，将其发送给从节点，并使用缓冲区中保存的写操作命令来更新从节点的数据。
之后从节点会周期性地向主节点发送 PSYNC 命令来获取更新的数据。
4、Redis 主从复制可以提高系统的可用性和扩展性。例如，在主节点故障时可以快速切换到某个从节点继续提供服务，同时从节点也可以用于读取数据以减轻主节点的负载。

搭建Redis集群可以提供Redis的高可用性
1、主从：数据备份，主宕机了就挂机了，一主多从但是容量没有增加
2、主从+哨兵：数据备份，有了就可以选举了，但是容量依然没有增加。
3、ReidsCluster(无中心，16384hash槽)，容量有增加的，可以针对任何一个节点做主从+哨兵。

Elasticsearch
*es的倒排索引
ES的倒排索引（Inverted Index）是一种用于快速检索文档的索引结构。它将每个文档中出现的每个单词（或词组）都映射到包含该单词（或词组）的文档列表中，这种映射关系称为倒排索引。
倒排表以关键字进行索引，关键字所对应的表项中记录了出现这个字的所有文档以及ID，字符出现文档的位置。

总之，ES的倒排索引是一种高效的文本检索和分析索引结构，通过将每个文档中出现的单词（或词组）映射到包含该单词（或词组）的文档列表中，可以将查询的关键词与文档的索引进行匹配，从而只搜索包含关键词的文档，大大减少了搜索范围，实现了快速的文本检索和高级查询操作。 


ElasticSearch  的实现主要包括以下几个方面：
1.  分布式存储：ElasticSearch  的数据存储是分布式的，可以在多台服务器上进行分片存储和分布式查询。 
2.  倒排索引：ElasticSearch  使用了  Lucene  的倒排索引技术，可以快速地定位文档和关键词之间的关系，从而进行高效的搜索和查询。 
3.  分词和分析：ElasticSearch  针对多语言和不同领域的文本数据，采用了多种不同的分词和分析技术，包括字词分割、断词处理、同义词处理等，可以提高搜索的准确性。 
4.  多种数据类型支持：ElasticSearch  支持多种数据类型，包括数字、文本、日期、地理位置等，可以满足不同场景下的数据存储和查询需求。 
ES的评分机制
ES可以通过关键字来检索，检索出的数据是按照分数来排序的，分数越高，位置就靠前。
1、评分算法:使用 BM25 算法BM 是Best Match最佳匹配的缩写，25指的是第25次算法迭代。
一个文档的相关度评分取决于每个查询词在文档中的 权重 ，这个权重主要有一下三部分组成。
2、权重组成
● 检索词频率：该词出现的频率越大，评分越高
● 反向文档频率：该词如果在该索引(表)中出现的频率很高，评分会随之降低也就是说，假如索引一共有 10 个数据，9个都含有 hello ，评分会降低
● 字段长度准则：字符长度越长，相关性越低，也就是短文本中匹配到比长文本匹配得分更高


ES分词器有哪些
1、Standard Analyzer：
标准分词器，也是ES的默认分词器，按词切分，小写处理，非字母会处理。
2、Simple Analyzer：
按照非字母切分，非字母都会被去除，即只处理字母，小写处理。
3、Whitespace Analyzer:
按照空格切分。
4、Stop Analyzer：
相对于Simple Analyzer多了stop filter，会把is，a，the等无语义的词去除，即含有停用词。
5、Keyword Analyzer：
不分词，直接将输入的文档当做一个词输出。
6、ICU Analyzer
中文分词器，由于提供了Unicode编码支持，能够更好地支持中文。需要通过插件安装才可以使用。
7、IK Analyzer
中文分词器，使用较多，需要通过插件安装才可以使用。

IK中文分词器如何分词
将一段文字进行IK分词处理，主要逻辑包括三部分：
1）加载词典：加载内置或者扩展词典。
2）词的匹配(分词)：有了词典之后，就可以对输入的字符串逐字句和词典进行匹配
3）消除歧义：通过词典匹配出来的切分方式会有多种，消除歧义就是从中寻找最合理的一种方式
1.加载词典：
词典的配置其实是非常重要的，ik为我们提供了三种常见的词典分别是：
1）main.dic：主词典，一些常用词
2）quantifier.dic：常用的量词(平方米，年代，时间，阶段)
3）stopword.dic：停用词
4) ext.dic：扩展词库

这些词典用户都可以自行扩展，只需要配置IKAnalyzer.cfg.xml文件即可 词典是怎么加载的呢，常用的字典树 tire tree 是一种结构简单的树，也叫做前缀树，结构如下图所示

	
从根节点出发，把一个词挂在这颗树上，红色的节点表示词的结尾。上图中间表示，前门是一个词，门是结尾；前门巨虎也是一个词，前门拒不是一个完整的词，因为拒不是结尾。

2.词的匹配:
分词器分词有两种模式：
● smart模式(智能)：拆分出一个更合理的结果，所以这里就需要歧义判断了。	
● 非smart模式（细粒度)：非smart模式可以认为是最小力度的分词，把分出来的词全部输出。如我要买手机 会分出 买手和要买
从非smart的分词结果中可以看出，对于一个语句可以有很多种切分方式，非smart就是把每种可能的分词结果都给出来了。
而smart模式，就是需要在这几种分词模式中，寻找一种认为最合理的分词方式。
从处理角度说，设置了smart模式，就是在进行词切分后，在进行一次分词的选择，即通常说的消除歧义

3、消除歧义
消除歧义只有在Smart模式才会生效，IK歧义判断规则如下，优先级从上到下一致降低：
1.分词文本长度越长越好
2.分词个数越少越好
3.分词路径跨度越大越好
4.分词位置越靠后的优先
5.词长越平均越好
6.词元位置权重越大越好


4.ES有使用过吗？怎么去添加数据？
ES支持RestFul风格的请求，在我们项目中使用高级API来操作ES。首先添加数据的时候使用PUT方式。在添加数据的时候ES会自动分配一个数据的ID
1、使用API连接ES
2、创建ES的客户端
3、给客户端设置操作的索引和类型
4、把插入的数据转成JSON字符串
5、调用API进行插入操作
6、ES返回的也是JSON数据(或者map)
7、把返回的JSON数据解析成Java实体类对象(查询）

5.es和solr有什么区别
es适合动态数据，比如动态日志的搜索
solor适合静态数据(死数据)
相同点：都是基于apche下面的搜索引擎框架开发的(Lucence)





命令



Maven
常用的maven的命令
mvn clean	对项目进行清理，删除target目录下编译的内容
mvn compile	编译项目源代码
mvn test		对项目进行运行测试
mvn package	打包文件并存放到项目的target目录下，打包好的文件通常都是编译后的class文件
mvn install	在本地仓库生成仓库的安装包，可供其他项目引用，同时打包后的文件放到项目的target目录下

什么是maven的传递依赖
Maven 的依赖传递机制是指：不管 Maven 项目存在多少间接依赖，POM 中都只需要定义其直接依赖，不必定义任何间接依赖，Maven 会动读取当前项目各个直接依赖的 POM，将那些必要的间接依赖以传递性依赖的形式引入到当前项目中。Maven 的依赖传递机制能够帮助用户一定程度上简化 POM 的配置。





git
说几个常用的git命令
git init: 在目录中初始化一个新的Git代码库
git add <file>: 将文件添加到暂存区
git commit -m "<message>": 提交文件到Git代码库，并附带一条提交信息
git status: 显示当前工作目录的状态，包括哪些文件被修改或添加到暂存区
git log: 查看提交历史记录
git pull: 从远程仓库拉取最新的代码到本地
git push: 将本地的代码推送到远程仓库
git clone: 克隆一个现有的Git代码库到本地




RabbitMQ

消息队列的主要目的
解耦、异步 、削峰填谷
消息中间件的使用场景：1、应用的解耦；2、异步处理；3、流量的削峰；4、日志处理；5、纯粹的消息通信。
为什么使用消息中间件
解决分布式系统之间消息的传递。
rabbitMQ中如何保证消息的顺序消费
保证顺序消费的原因是如果顺序出错执行的结果也可能会出错。
解决方式：
可以采用单线程，但是消费速度满了，网络问题也会造成影响。
可以通过在每个消息上再绑定一个消息，每个消息都知道上一个消息是谁，消费时会判断上一个消息是否消费，如果消费了就正常消费，并记录在redis中



mq的实现流程

MQ消息的发送过程
提供者连接MQ-->创建Channel-->提供者发送消息-->交换机-->队列-->消费者。

MQ消息的传播机制
1、简单队列:一个生产者对应一个消费者
2、work 模式:一个生产者对应多个消费者，但是一条消息只能有一个消费者获得消息！！！
3、发布/订阅模式:一个消费者将消息首先发送到交换器，交换器绑定到多个队列，然后被监听该队列的消费者所接收并消费。主要解决解决的问题是一个消息需要让多个消费者消费。
4、路由模式:
生产者将消息发送到direct交换器，在绑定队列和交换器的时候有一个路由键，消息可以根据不同的路由键转发到不同的队列。也就是让消费者有选择性的接收消息。
5、主题模式
上面的路由模式是根据路由key进行完整的匹配（完全相等才发送消息），这里的通配符模式通俗的来讲就是模糊匹配。
6、RPC模型
远程调用
客户端通过发送消息给队列，服务端通过监听这个队列获取消息---》做到客户端调用服务端，并且传递了数据。
服务端发送消息给队列，客户端监听队列获取消息---》做到获取返回值。

2、消息的传递过程
rabbitmq：生产者--》交换机--》队列--》消费者
kafka：生产者--》topic--》分区---》消费者


消息的重复消费

1、为什么会出现消息的重复消费
消息已经被消费了(插入到数据库)，但是没有手动ACK(ACK的时候宕机了)，MQServer认为没有消费(从答应打转到待分配)，又再次的转给了其他的消费者(第二次 插入)。

2、如何解决这个问题呢？
	这个问题和接口的幂等性是同一个问题。需要保证消息幂等性就行，只需要给发送的消息中携带一个Token即可，消费者先判断Tokken是否有效，如果有效正常消费，如果无效此时需要手动ACK。

MQ如何防止数据丢失
持久化消息：将消息持久化到磁盘上，即使在系统崩溃或重启后，也能够恢复消息。
冗余备份：使用多个消息代理节点以及复制备份机制，确保消息在任何情况下都不会丢失。
监控和报警：实时监控MQ的运行状态，一旦发现异常情况及时报警并采取相应的措施，防止数据丢失。






Kafka和RabbitMQ的区别：
1.  数据持久化方式：Kafka将所有消息持久化到磁盘上，而RabbitMQ默认情况下只将未被确认的消息存储在内存中。 
2.  消息传递模式：Kafka采用发布/订阅模式，消息可以被多个消费者进行消费；而RabbitMQ采用队列模式，每个消息只能被一个消费者接收。 
3.  性能：Kafka在大数据量的实时处理和流式处理，RabbitMQ适合于异步通信和高可靠性的消息传递。 
4.  编程语言支持：Kafka主要使用Java编写，但也提供了其他语言的客户端API；而RabbitMQ支持多种语言的客户端API。 

Kafka

架构

Broker：Kafka集群中运行的服务器节点。
Topic：消息发布的类别或主题。
Producer：向Kafka broker发送消息的应用程序。
Consumer：从Kafka broker接收消息的应用程序。
Partition：一个topic被分成若干个partition，每个partition对应一个有序的、不可变的消息序列。
Offset：Kafka为了保证partition的顺序和唯一性，给每个message在partition中分配的唯一标识。
Consumer Group：多个consumer组成的一个组。
Replication：为了提高系统可靠性，Kafka将每个partition复制到多个broker上，这个过程就称为replication。

Kafka和Zookeeper关系
Kafka依赖Zookeeper来处理以下任务：
1.  Broker协调。Kafka将使用Zookeeper来维护存储Broker的信息（例如：Broker  ID，主机和端口）。当Broker加入或退出Kafka集群时，Zookeeper将作为协调器来维护Broker状态。
2.  Topic管理。Kafka将使用Zookeeper来维护存储所有Topic的信息（例如：分区分布，副本分布等）。这些信息是在Zookeeper上开辟的节点（znode）中存储的。
3.  Consumer  offset存储。Kafka中的每个Consumer组都需要跟踪它们读取的位置（也称为“消费者偏移”）。这是在Kafka中跟踪的，但是偏移量是保存在Zookeeper中的。每个消费者都有一个独立的Zookeeper节点，用于跟踪他们的偏移量。

综上所述，Zookeeper在Kafka中扮演了一个重要的协调器的角色，用于管理多个Broker，Topic，和消费者。同时，它也负责存储和更新元数据和状态信息，以确保Kafka集群和消费者组都能正常运行。

Kafka选举
当leader副本宕机或者无法正常工作时，需要选举新的leader副本来接管分区的工作。
过程：
1、每个参与选举的副本会尝试向ZooKeeper上写入一个临时节点，表示它们正在参与Leader选举;
2、所有写入成功的副本会在ZooKeeper上创建一个序列号节点，并将自己的节点序列号写入该节点;
3、节点序列号最小的副本会被选为新的Leader，并将自己的节点名称写入ZooKeeper 上的/broker/.../leader节点中。


Kafka的重平衡机制
Kafka的重平衡(Rebalance)机制是指在消费者组中新增或删除消费者时，Kafka 集群会重新分配主题分区给各个消费者,以保证每个消费者消费的分区数量尽可能均衡。
大致的步骤如下:
1、暂停消费:在重平衡开始之前，Kafka 会暂停所有消费者的拉取操作,以确保不会出现重平衡期间的消息丢失或重复消费。
2、计算分区分配方案: Kafka 集群会根据当前消费者组的消费者数量和主题分区数量，计算出每个消费者应该分配的分区列表，以实现分区的负载均衡。
3、通知消费者: 一旦分区分配方案确定, Kafka 集群会将分配方案发送给每个消费者，告诉它们需要消费的分区列表,并请求它们重新加入消费者组。
4、重新分配分区:在消费者重新加入消费者组后，Kafka 集群会将分区分配方案应用到实际的分区分配中，重新分配主题分区给各个消费者。
5、恢复消费:最后, Kafka会恢复所有消费者的拉取操作，允许它们消费分配给自己的分区。


Kafka如何保证消息不丢失
消息持久化：Kafka将消息写入磁盘以进行持久化存储。这样即使在发生故障和重启之后，仍然可以从磁盘中读取未被消费的消息。

复制机制：Kafka使用副本机制来确保消息的可靠性。副本是指同一份数据的备份，它们位于不同的节点上。当一个节点宕机时，其他节点上的副本会接替宕机节点的工作，并继续提供服务，因此即使某个节点宕机，也能够保证数据的可靠性。



其他
什么是RestFul风格
Restful风格是一种软件架构风格，通常用于设计网络应用程序。它是基于HTTP协议的，并使用HTTP方法来实现对资源的增删改查操作。使用Restful风格的应用程序将资源的状态和操作转换成对RESTful API（Representational State Transfer Application Programming Interface）的调用，使代码实现更加简单和易于管理。Restful风格的应用程序还具有更好的可伸缩性和可重用性，更适用于分布式系统 

1.Restfuel风格它指的是请求接口的方式
资源的定位(url)，资源的操作（mehod） ==》要访问哪块资源，对这个资源做一个什么操作
举例：
http://localhost:8080/user -- GET -->查询

2.Restfuel风格有什么好处?
从请求地址+请求方式就可以知道对资源做一个什么样的操作。
请求更加的清晰，简单

3.SpringMVC支持Restful风格吗
@GetMapping
@PostMaping
@PutMapping
@DeleteMapping
@PathVariable

锁的分类，分布式锁和本地锁的区别


悲观锁
悲观锁认为对同一个数据的并发操作，默认一定会发生修改。所以通常采取加锁的方式，悲观地认为不加锁的并发操作一定会出问题。
synchronized 是悲观锁的实现，因为 synchronized 修饰的代码，每次执行时会进行加锁操作，同时只允许一个线程进行操作，所以它是悲观锁的实现。

乐观锁
乐观锁正好和悲观锁相反，它获取数据的时候，并不担心数据被修改，每次获取数据的时候也不会加锁，只是在更新数据的时候，通过判断现有的数据是否和原数据一致来判断数据是否被其他线程操作，如果没被其他线程修改则进行数据更新，如果被其他线程修改则不进行数据更新。

独占锁、共享锁
独占锁是指该锁一次只能被一个线程所持有；共享锁是指该锁可被多个线程所持有。
对于Java ReentrantLock而言，是独占锁。
对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。
读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。
对于Synchronized而言，当然是独享锁。

互斥锁/读写锁
 上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。互斥锁在Java中的具体实现就是ReentrantLock；读写锁在Java中的具体实现就是ReadWriteLock。

可重入锁/不可重入锁
可重入锁（Reentrant Lock）允许同一个线程多次获得该锁，而不会导致死锁，也就是说，当当前线程持有锁时，可以再次请求该锁而不会被阻塞。
不可重入锁（Non-Reentrant Lock）则不支持同一个线程多次获取该锁，如果尝试获取已经持有的锁，则会被阻塞,导致死锁.

公平锁/非公平锁
 公平锁是指多个线程按照申请锁的顺序来获取锁。非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。

本地锁、分布式锁
本地锁：只能锁当前服务器
分布式锁：服务器集群时，两个请求分配到不同机器，都用各自本地锁会出现锁失效的问题。需要用redis来实现分布式锁

分布式如何实现？
redis：key不允许重复，setnx+lua脚本或者使用redisson实现。
zk：节点名称不允许重复，

拿到锁后宕机了怎么办？宕机了导致锁无法释放。
给锁设置生存时间

锁的过期时间设置多久呢？如果请求还未处理完呢？结果锁过期自动释放了。
看门狗机制，加锁后启动一个定时任务来进行续期，锁手动释放时就停止续期


*lock和syn在项目中如何运用的
synchronized:是Java中的关键字，是一种同步锁。用来保证被它修饰的方法或者代码块在任意时刻只能有一个线程调用。



synchronized的特点
1、原子性：原子性的操作执行到一半，并不会因为CPU线程调度而被打断。
2、可见性：释放锁所有的数据都会写回内存，获取锁都会从内存中读取最新数据。
3、可重入锁：它是一个可重入锁
4、重量级锁：他是一个重量级锁，开销很大，开发过程中尽量少用。
应用场景
修饰一个代码块、修饰一个方法 、修改一个静态的方法、修改一个类


lock
1.获取锁对象Lock lock = new ReentrantLock();
2.加锁 lock.lock()
3.释放锁lock.unlock();

项目中建议使用lock锁，LOCK锁手动释放了很灵活，而且还可以终端正在等待获取锁的线程，它里面提供了各种锁，锁的种类很丰富。
在项目中，lock锁通常用于多线程的并发控制，以保证共享资源的安全访问。
lock锁是多线程场景下常见的并发控制手段，可以通过锁定共享资源、线程同步、数据缓存和任务调度等方式来保证线程安全和数据一致性。但是需要注意，锁的使用需要谨慎，过多的锁会影响系统的性能，需要根据具体场景进行优化和调整。秒杀业务、下单业务、支付业务

锁synchronized与lock的区别

1、synchronized是java关键字，jvm层面； lock是类；

2、synchronized无法获取到锁的状态， lock可以获取到锁的状态；

3、synchronized是自动锁，如果代码执行完毕或发生异常， 他会自动释放锁的资源， lock手动锁，需要人工进行调用unlock方法进行释放锁的资源， 一般都是放在finally； 如果不释放会发生死锁；

4、synchronized如果线程a获取到锁的资源，发生阻塞，线程b会一直等待， 而lock如果获取不到锁的资源，线程不用等待就结束了；

5、synchronized是可重入、不可中断， 非公平， lock 可重入、 可判断、 可公平（两则皆可）；
**sentinel的流控规则有哪些？
QPS（每秒查询率）
系统参数，
熔断降级
热点参数，
黑白名单
1. 直接限流规则：基于QPS（每秒查询率）对接口进行限流，可配置预热时间窗口、最大并发数等参数，主要用于限制系统的整体流量。

2. 关联限流规则：基于关联的资源信息（例如URL参数、服务调用关系等）对接口进行限流，可配置QPS、关联维度等参数，主要用于防止恶意请求和保护关键业务资源。

3. 流量整形规则：对于不稳定或流量较大的接口，可以使用流量整形规则限制其流量，实现平滑的流量控制，避免系统崩溃或雪崩。

4. 排队等待规则：对于最终需要等待的接口，可以使用排队等待规则对请求进行排队处理，实现有序的请求服务，避免竞争占用资源。

5. 系统保护规则：用于防止系统出现意外情况时引发的降级问题，例如CPU负载、平均响应时间、最大并发数等指标的监控和保护。

lua脚本的作用
Lua是一种轻量级的编程语言，可以用于各种业务逻辑的处理、配置文件解析、请求响应处理等众多场景中
1. 配置文件解析：Lua脚本可以用于解析和执行应用程序的配置文件。通过编写Lua脚本，可以实现配置文件中的各种逻辑和处理，大大提高了应用程序的灵活性和可配置性。
2. 业务逻辑处理：Lua脚本也常常用于处理各种业务逻辑。在网络应用中，Lua脚本可以实现路由、分流、负载均衡、数据处理等功能。
3. 请求响应处理：在网络请求和响应的处理中，Lua脚本可以被用来处理特定的请求和响应。例如，Lua脚本可以根据URL路由请求、生成响应内容、操纵HTTP头信息等功能。
4. 执行动态代码：Lua脚本可以在应用程序运行时编译和执行代码字符串。这种方法可以让应用程序像扩展自身一样运行，从而增强应用程序的灵活性。
5. 实现插件机制：Lua作为一种脚本语言非常适合用来实现插件机制。通过Lua脚本，可以让应用程序变得更容易扩展和集成，为用户提供出色的用户体验和个性化需求的满足。

Lua脚本可以保证原子性，因为Redis会将Lua脚本封装成一个单独的事务，而这个单独的事务会在Redis客户端运行时，由Redis服务器自行处理并完成整个事务，如果在这个进程中有其他客户端请求的时候，Redis将会把它暂存起来，等到Lua脚本处理完毕后，才会再把被暂存的请求恢复。




项目
登录
项目的登录模块如何实现？

1、前端发送请求，携带手机号和验证码
2、服务端接收请求，校验数据
3、先对比它的校验是否和手机号匹配，正确
4、判断这个手机号是否存在
	a)如果不存在说明是第一次登录，自动实现注册
		1)注册的本质就是给用户表中插入一条记录
		2)用户名取手机号
		3)登录用户登录成功了进入个人详情页面去修改(名字，邮箱，头像，密码)
5)如果手机号已经存在实现登录
	a)生成一个token(uuid),把token作为key，用户的信息作为value保存在redis中
	b)把token响应给前端
	c)前端接收到token后保存在本地，下次请求需要携带就可以了


在用户模块，对于用户密码的保护，通常都会进行加密。我们通常对密码进行加密，然后存放在数据库中，在用户进行登录的时候，将其输入的密码进行加密然后与数据库中存放的密文进行比较，以验证用户密码是否正确。 目前，MD5和BCrypt比较流行。相对来说，BCrypt比MD5更安全。因为其内部引入的加盐机制

        /**
         * 得到盐
         * 盐是一个随机生成的含有29个字符的字符串,并且会与密码一起合并进行最终的密文生成
         * 并且每一次生成的盐的值都是不同的
         */
String gensalt = BCrypt.gensalt();
System.out.println("salt:"+gensalt);
String saltPassword = BCrypt.hashpw("123456", gensalt);


Token和JWT

Token：令牌， 是访问资源的凭证。
JWT：json web token缩写。它将用户信息加密到token里，服务器不保存任何用户信息。服务器通过使用保存的密钥验证token的正确性，只要正确即通过验证。
JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名。
头部：用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个JSON对象。
载荷：载荷就是存放有效信息的地方。
jwt的第三部分是一个签证信息，这个签证信息由三部分组成：这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。


微服务鉴权

1. 用户登录成功，后台管理微服务签发JWT TOKEN信息返回给用户
2. 用户再次进入网关开始访问，网关过滤器接收用户携带的TOKEN
3. 网关过滤器解析TOKEN ，判断是否有权限，如果有，则放行，如果没有则返回未认证错误
用户的登录信息是保存在哪里的
以token为key 以string的形式保存在redis当中

用户发送手机号后，返回的验证码存在哪
以手机号加验证码为key值，以string的形式保存在redis当中


发送短信接口有几个参数
手机号码、验证码类型、验证码有效期、随机验证码、短信模板、短信签名 

怎么实现单点登录;同一个IP下,一个账号已经登陆了,怎么确保登录的唯一性;
1、什么是单点登录？
我在订单模块登录过了，在调用购物车模块的时候就不需要再登录了。
2、如何实现单点登录？
登录成功返回一个token，每次请求携带这个token，在网关中统一校验token，校验成功后把这个token继续写到请起头中，路由到具体的服务器。服务中写一个拦截器从根据token获取用户信息，放入到ThreadLocal中，后面的组件就可以使用了。
3、如何确保登录唯一性？
 在服务端保存登录的用户信息，下次再登录的时候先判断一下这个账号是否在线。

多端登录怎么实现的？有了解过OAuth2.0吗？
多端登录：同一个账号在多个设备上同时登录(app,web,小程序)，而不是多个手机同时登录。

怎么实现？
a、只要认证就返回一个token给它(客户端)，只要token是合法的就执行，这样下去就会出现token无法管理了。
b、如果需要做账号登录状态后台查询，token就要保存在服务端。
c、生成token的时候需要把设备号揉入进去

什么是OAuth2.0协议
专门做第三方认证的，比如微信登录。
场景：我现在要登录京东，使用微信号登录京东就是一个第三方认证的。
首先京东是不知道微信密码，微信是知道的，我现在登录的是京东。
京东会把我引到微信的一个授权页面（微信在问我？你的微信号是否要授权给京东）
如果点击是了，微信来认证我的账号，微信认证成功后需要通知京东，然后把用户的信息方过去。













搜索

查询和展示


ES实现搜索模块的流程通常如下：
1. 索引数据：将需要搜索的数据以JSON格式的方式存储在ES的索引中，建立文档和字段对应关系。
2. 接收搜索请求：用户在系统中输入搜索关键字后，ES搜索模块接收到搜索请求。
3. 分析搜索请求：ES搜索模块对搜索请求进行分析，并利用分析器，进行分词、过滤、词性还原、同义词等处理，生成匹配查询。
4. 执行查询：ES基于Lucene的查询语句，进行基于倒排索引的搜索，并根据得分对搜索结果进行排序。
5. 过滤查询：根据用户过滤条件，对数据进行继续的筛选和过滤，去除不符合要求的数据。
6. 数据分页：将过滤后的数据按照分页规则，拆分成多个数据页进行展现。
7. 返回搜索结果：将搜索结果以可视化的形式返回给用户，包括搜索关键字、搜索结果总数、当前页码、每页展示数据条数、查询的时间等信息。


热搜词：对一段时间内用户输入的搜索词进行一个数量统计，把频率高的词语显示给用户。这里的话会涉及到一个实时统计实时更新，我采用的是比较轻量化的kafka stream，他有一个滑动窗口方便我们设置一个时间，并且可以统计出现的次数。我们可以把这个搜索词和出现的次数存到redis中。redis中采用zset集合类型存储，因为他有一个评分，我们可以把出现的次数当作评分来排序，只需要拿出当前时间段排名靠前的几个热搜词。
这里还有一个问题，就是对于这个相似的词语如何统计的问题，我们都知道输入相似的词语不能算作两个词，而是应该算作1个词出现了两次。我的一个解决办法是把这个词传到kafka的stream之前，先到es中进行一个相似词语的匹配，这里就用到了es的more_like_this相似查询。如果有相似的词语，就把这个相似的词语返回给stream拿去统计并存储，如果没有相似的词语，那就把原词语返回拿去统计

项目中ES的使用？
ES的倒排索引是一种高效的文本检索和分析索引结构，通过将每个文档中出现的单词（或词组）映射到包含该单词（或词组）的文档列表中，可以将查询的关键词与文档的索引进行匹配，从而只搜索包含关键词的文档，大大减少了搜索范围，实现了快速的文本检索和高级查询操作。 
es如何实现热搜
热搜词：对一段时间内用户输入的搜索词进行一个数量统计，把频率高的词语显示给用户。这里的话会涉及到一个实时统计实时更新，我采用的是比较轻量化的kakfa stream，他有一个滑动窗口方便我们设置一个时间，并且可以统计出现的次数。我们可以把这个搜索词和出现的次数存到redis中。redis中采用zset集合类型存储，因为他有一个评分，我们可以把出现的次数当作评分来排序，只需要拿出当前时间段排名靠前的几个热搜词。
这里还有一个问题，就是对于这个相似的词语如何统计的问题，我们都知道输入相似的词语不能算作两个词，而是应该算作1个词出现了两次。我的一个解决办法是把这个词传到kafka的stream之前，先到es中进行一个相似词语的匹配，这里就用到了es的more_like_this相似查询。如果有相似的词语，就把这个相似的词语返回给stream拿去统计并存储，如果没有相似的词语，那就把原词语返回拿去统计

在Elasticsearch中实现热搜功能一般需要以下几个步骤：

1.  收集用户搜索关键词信息：可以通过在客户端应用程序中记录用户的搜索关键词和搜索时间等信息，然后将这些信息存储到Elasticsearch中。 
2.  定义搜索关键词的索引映射：需要定义一个包含关键词和搜索时间等字段的索引映射，在其中设置相应的类型以便能够进行聚合操作。 
3.  使用聚合统计搜索次数：使用Elasticsearch的聚合功能，对已经存储的搜索关键词信息进行聚合分析，从而统计每个关键词的搜索次数。 
4.  对搜索次数进行排序：完成统计后，对搜索次数进行排序，得出热搜排行榜。 
5.  前端页面展示：将热搜排行榜数据返回给前端应用程序，让用户可以看到最热门的搜索关键词。 
总之，要实现热搜功能，就是收集、存储、聚合和排序。



要实现热搜，可以结合 Elasticsearch 中的聚合（aggregations）功能和定时任务（cron job）来达到自动更新热门搜索词汇的效果。

具体操作如下：

1. 创建一个索引，包含需要被搜索的字段。例如：

PUT /my_index
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "content": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "created_at": {
        "type": "date"
      }
    }
  }
}

这里使用了中文分词器 ik_max_word，可以对中文文本进行分词处理。

2. 使用定时任务来更新热门搜索词汇。使用 Elasticsearch 的聚合功能，找到搜索次数最多的前 N 个关键词。例如：

POST /my_index/_search
{
  "size": 0,
  "aggs": {
    "hot_keywords": {
      "terms": {
        "field": "title.keyword",
        "size": 10
      }
    }
  },
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "created_at": {
        "order": "desc"
      }
    }
  ],
  "range": {
    "created_at": {
      "gte": "now-7d/d",
      "lte": "now/d"
    }
  }
}

这里使用了 terms 聚合器，以 title.keyword 字段作为聚合字段，找到出现次数最多的前 10 个关键词。同时，使用 sort 排序，按照创建时间倒序排列，以确保更新后的搜索结果是最新的。range 过滤器可以限制搜索结果只包含过去 7 天内的内容。

3. 将搜索次数最多的前 N 个关键词保存在一个 Redis 或者 Memcached 缓存中，以供应用程序显示


订单


项目中订单模块
要实现订单功能，通常需要以下步骤：
订单模块的流程一般如下：
1. 用户提交订单：用户在系统中浏览商品，将需要购买的商品加入购物车，确认订单后提交订单。
2. 订单接收：系统接收到用户提交的订单，生成唯一订单号和订单数据。
3. 库存校验：系统根据订单中的商品详情，检查数据库中对应商品的库存情况，如果库存充足，继续执行下一步；否则，返回库存不足的提示信息给用户。
4. 订单金额计算：根据订单中的商品及其数量，计算订单金额，并根据使用的优惠券、积分、折扣等信息进行相应折扣计算。
5. 收货地址确认：确认用户的收货地址，并在订单中记录。
6. 支付：根据用户选择的支付方式，将订单金额发送给第三方支付平台进行支付操作，如果支付成功，继续执行下一步；否则，返回支付失败信息给用户。
7. 订单状态更新：支付成功后，更新订单状态为已支付，并记录支付时间和渠道等信息。
8. 订单完成：如果用户购买的商品需要物流配送，等待商户生成物流单号并为订单进行发货操作；如果没有物流配送，订单完成。

业务层实现逻辑：
1.获取所有购物项
2.统计计算：总金额，总数量
3.填充订单数据并保存
4.获取每一个购物项保存到orderItem
5.删除购物车中数据

上面操作只实现了下单操作，但对应的库存还没跟着一起减少，我们在下单之后，应该调用商品微服务，将下单的商品库存减少，销量增加。每次订单微服务只需要将用户名传到商品微服务，商品微服务通过用户名到Redis中查询对应的购物车数据，然后执行库存减少，库存减少需要控制当前商品库存>=销售数量。


订单id生成策略 redis生成主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。

订单生成服务对应的技术点
1、唯一性生成算法，例如UUID、Snowflake(雪花算法)等,生成唯一的ID号,可以保证在分布式系统中的唯一性和顺序性。
2、将订单号存储在缓存系统中，例如Redis等，避免频繁地生成订单号。
3、采用分布式锁等机制，避免多个请求同时生成同一个订单号。
4、设计一个高性能的生成器，能够支持高并发的生成订单号请求,例如采用多线程、步等方式提高系统的性能和响应速度。
5、对于生成失败的订单号请求，可以采用重试机制，避免因网络或者其他因素导致的生成失败。


rabbitmq如何处理订单超时？
Redisson
使用Redisson的RLock对象来获取一个分布式锁，设置一个适当的过期时间，以确保锁不会永久存在，在订单处理开始时获取锁，在订单处理完成后释放锁。如果订单在指定时间内没有得到处理，则锁将自动过期并释放，从而确保其他线程可以继续处理该订单。

RabbitMQ延时插件
通过自定义交换机，然后绑定队列，发送消息时指定消息过期时间实现延迟，跟延迟队列的区别是：
1、过期中的消息这边是暂存在交换机，那边暂存到队列中；
2、消息过期不会像TTL那样等待执行。

超卖问题
多个秒杀集群同时请求时，需要用分布式锁，所有请求先去Redis加锁→秒杀→解锁，实际上多个IIS请求最终到数据库 依次先后进行的

使用reids的 watch + multi + setnx  指令实现
在 Redis 中，可以通过使用 SETNX 命令来构建锁，将对应锁的 key 值获取到的 uuid 结果进行判断验证去释放锁，获取锁的时候加上锁的超时功能

利用数据库的行级锁，用数据库运算防止商品超卖


订单业务中经常涉及到的表参数包括：
1. 订单表
用于记录订单基本信息，包括订单号、用户ID、订单金额、下单时间、收货地址、订单状态等。
2. 订单详情表
记录订单中每个商品的详细信息，包括商品ID、商品名称、商品价格、购买数量、优惠信息等。
3. 商品表
记录商家提供的商品基本信息，包括商品ID、商品名称、商品价格、库存量、销售量等。
4. 用户表
记录用户的基本信息，包括用户ID、用户名、手机号、地址等。
5. 支付表
记录订单的支付信息，包括支付方式、支付时间、支付金额等。
6. 物流表
记录订单的物流信息，包括快递公司、运单号、发货时间、到达时间等。


购物车
我们实现的是用户登录后的购物车，用户将商品加入购物车的时候，直接将要加入购物车的详情存入到Redis即可。每次查看购物车的时候直接从Redis中获取。

redis存入形式hash：存储一个对象数据的(对象理解为map也行)。 大key为用户id 小key为sku，数量num

1.根据id获取sku数据 ( 库存量)
2.定义feign接口,并定义查询方法
3.订单服务添加依赖
4 订单服务启动类添加feign接口扫描
5.添加控制类和服务类
购物车数据存在redis,商品价格更改了怎么办

1. 购物车里面只保存商品的 id。
2. 商品的价格按照 id 单独存在 redis 里面。
3. 价格改动的时候，按照商品 id 修改 redis 里面的价格数据。
4. 获取购物车信息的时候，根据购物车里的商品再单独在 redis 里面查询商品价格

将购物车数据从 Redis 存储到 MySQL 中
异步消息队列
通过异步的方式将 Redis 中的购物车数据发送到消息队列中，再通过消费消息的方式将购物车数据写入 MySQL 中。这种方式可以实现实时性较高的导入，在用户下单、购物车更新等操作时都可以进行数据同步。消息队列和消费者的性能要求比较高，需要使用高效的消息队列、消费者和数据库连接池等技术进行优化。
在进行数据存储时，需要根据购物车的数据结构和业务场景进行合理的表设计和索引设计，以及适当的数据缓存策略。同时，要保证数据同步的准确性和完整性，避免因数据不一致或重复等问题影响系统功能和用户体验。

其他
并发的场景下使用分布式事务的冲突问题
1.什么情况下会并发
大量请求发送过来，如秒杀场景，所以处理速度一定要快

2.如何解决并发问题
在redis中快速处理请求，通过mq异步通知数据库保证最终一致性

3.分布式事务和本地事务的区别
分布式事务：是指跨多个数据库或应用程序的事务，需要保证在分布式系统中的多个节点上执行的多个事务操作具有ACID属性
本地事务：是指在单个数据库或应用程序中执行的事务

区别：
范围不同：本地事务只涉及到单个数据库或应用程序；而分布式事务涉及到多个数据库或应用程序，
复杂度：需要保证在分布式系统中的多个节点上执行的多个事务操作具有ACID属性，处理跨网络和多个节点之间的事务提交、回滚、超时、故障处理等问题，实现复杂度比本地事务要高得多。

*4.如何解决
消息队列：通过使用事务消息队列，可以将事务操作和消息传递结合起来，从而保证事务的一致性和可靠性。

乐观锁：每个事务都带着版本号进行操作。如果两个事务同时尝试更新相同的数据，那么只有一个能够成功，另一个事务会被拒绝，并得到一个错误消息。这种方法适用于写入操作比较少的场景。

悲观锁：在执行任何修改操作之前，先要获得一个锁，以确保没有其他事务可以访问该资源。当多个事务尝试锁定同一项资源时，只有一个事务能够成功，其余事务将被阻塞。这种方法适用于写入操作频繁的场景。

分布式协调器：例如 zookeeper，它提供了一种可靠的方法来建立分布式锁。 在使用 zookeeper 控制锁时，每个参与者都必须连接到 zookeeper，以确保所有人都知道当前的上下文，并能够正确地处理数据。 这是一种比较可靠的方式，但也会引起性能问题和局部故障问题。



分布式事务在项目哪里用到了，分布式锁在项目哪里用到了
1. 分布式事务：分布式事务可以应用于涉及多个数据库或应用程序的业务场景，例如订单系统、库存管理系统、资金结算系统等。在这些场景中，需要保证跨多个节点的事务操作具有ACID属性来保证最终一致性，否则会出现数据不一致的情况。

2. 分布式锁：分布式锁可以应用于需要控制并发访问的业务场景，例如秒杀系统、并发的场景等。在这些场景中，需要保证同一时刻只有一个进程或线程能够持有锁，并防止锁的共享或重入问题。常见的分布式锁实现包括基于Redis的分布式锁、基于Zookeeper的分布式锁等。


消息积压处理
首先需要马上处理掉这些消息，可以通过多个消费端和线程来提高消费速度
然后找到积压原因：可能出现的原因有消费者宕机，网络和服务器问题等



项目中哪里用到了多线程
1、提高消费速度可以采用多线程，但是消息顺序乱了。
2、一个业务调用服务查询比较多可以使用多线程查询。

订单处理：当客户下单时，需要检查库存、扣除库存、生成订单号等等操作。使用多线程可以避免阻止主线程的延迟，让后台系统更快地完成这些任务。

并发请求：在电商平台上，有时需要同时发送很多请求来更新或抓取大量数据。通过将这些请求分配给多个线程，可以减少整体响应时间和服务器负载。


调用一个没有定义的接口，拦截器是否会被调用
拦截器拦截的是/**,postman访问一个/A接口，但是A接口没有定义，找不到对应的类就不会接着执行。
不会



子线程怎么获取到ThreadLocal中的数据
1.ThreadLocal:线程安全的本地容器，每个线程都一个
2.项目中怎么使用？
拦截器校验token，将用户信息保存在ThreadLocal中，控制层和业务层想要获取信息直接从ThreadLocal中获取
网关和服务时两个独立的服务，服务无法获取在网关中解析token放到ThreadLocal中的数据
3.底层：ThreadLocalMap，里面的entry数组来储存
4.为什么要用Entry数组
一个Threadl类中只有一个ThreadLocal，一个Thread代表一个线程，但是在一个线程中可能存在多个ThreadLocal，而ThreadLocal使用ThreadLocalMap来保存的，所以使用entry数组就是为了保存同一个线程中的多个ThreadLocal的。entry的key是ThreadLocal对象，value就是值。
5.什么情况下会出现内存泄漏
内存泄漏：也称作"存储渗漏"，用动态存储分配函数动态开辟的空间，在使用完毕后未释放，结果导致一直占据该内存单元。直到程序结束.一个对象使用完了，但是没有被GC回收，就会导致这块无法再被使用了。
1、使用完ThreadLocal后没有删除，一般使用完需要及时的删除。
2、线程一直在执行呢，没有结束。






项目中Lua脚本的使用
使用 Redis 缓存、Lua 脚本、token 令牌实现限流的实现流程如下：

1. 在 Redis 中创建一个计数器，用于记录当前时间窗口内已经处理的请求数量。
2. 在 Redis 中创建一个过期时间为时间窗口大小的 key，用于控制时间窗口的大小。
3. 在客户端获取一个 token 令牌，如果 Redis 中的计数器值小于阈值，则允许客户端请求，并将计数器加1；否则，拒绝请求。
4. 在 Lua 脚本中判断当前时间窗口是否过期，如果过期，则清空计数器；否则，返回当前计数器的值。
5. 将 Lua 脚本传递给 Redis，使用 EVAL 命令执行脚本，获得当前时间窗口内已经处理的请求数量。
6. 根据计数器的值和阈值判断是否允许客户端请求。
7. 返回请求结果，如果请求被拒绝，则可以返回一个错误码或错误信息，指示客户端该如何处理。

总之，使用 Redis 缓存、Lua 脚本、token 令牌实现限流可以有效地控制并发请求的数量，保护系统的稳定性和可用性。通过合理设置时间窗口大小和阈值，可以平衡系统的吞吐量和负载，提高系统的性能和效率。


限流 - 微服务的入口（路由网关 - 分布式限流 - redis）令牌桶算法

请求传到gateway网关当中，然后通过去redis中拿令牌， 把获取令牌的lua脚本发给redis来实现生成令牌

lua脚本的内容就是 根据两次生成令牌的时间间隔 乘以他的生成频率    具有原子性
如果生成的令牌足够，更新时间，请求通过
如果不够                 则更新时间，拒绝请求








设计一个能够支持高并发的系统需要考虑多方面的因素,包括架构、性能优化、容错和可伸缩性等，以下是一些一般性的建议和实践:

1.  分布式架构设计：将系统拆分为多个服务，每个服务可以独立部署和扩展，降低单一节点压力和单点故障的影响。
2.  缓存设计：采用合理的缓存策略，如缓存常用数据、缓存预热、缓存计算结果等，减轻数据库负载。
3.  负载均衡设计：通过负载均衡器将请求均衡分配到多台服务器上，实现水平扩展。
4.  数据库优化：使用合适的数据库技术，如分库分表、使用索引等，提高数据库性能。
5.  异步处理：将不需要实时响应的请求异步处理，如消息队列处理等，减轻服务器压力。
6.  高可用设计：采用多活或主备模式保证服务的高可用性，如异地多活、双机热备等。
7.  监控系统设计：建立监控系统，实时监测系统性能，及时发现和解决问题，保证系统稳定性。
8.  测试和评估：对系统进行充分的测试和评估，包括压力测试、性能测试等，发现潜在问题并进行优化
9、限流：可以对请求过来的流量做一波限流，保证放过去的请求足够真实的，倒三角。



常用设计模式

代理模式

它给一个对象提供一个代理，并由代理对象控制对原对象的引用。它使得用户不能直接与真正的目标对象通信。

代理对象类似于客户端和目标对象之间的中介，能发挥比较多作用，比如扩展原对象的能力、做一些切面工作（打日志）、限制原对象的能力，同时也在一定程度上面减少了系统的耦合度

工厂模式
电商模块中工厂模式通常用于创建和管理对象。具体来说，可以使用工厂模式创建与商品相关的各种对象，如订单、购物车、产品等。
工厂模式可以实现以下优点：
管理对象：使用工厂模式可以方便地将对象的创建和管理分离开来。由工厂类负责创建和管理实例，简化代码。
更好的封装性：将对象创建的过程放在一个函数内，确保了对象的创建行为被封装起来，从而降低了系统的耦合度。
灵活性：通过工厂模式，可以轻松地增加、删除或替换不同类型的产品，从而使系统更加灵活和可扩展。

*项目中的代理模式
通过代理对象来控制原对象的访问，可以在不改变原对象的情况下增强功能或进行权限控制。适合于管理开销较大的远程资源或者需对访问进行控制的场景。
缓存
电商系统中经常需要频繁读取和修改数据库中的数据，而这些操作会占用大量的系统资源和时间。为了提高系统性能和响应速度，可以采用代理模式，在代理对象中缓存已经读取过的数据或者是修改过的数据，当下一次请求时，可以直接从代理对象中获取缓存的数据，避免了重复的数据库查询和写入操作。

日志记录
电商系统中需要记录用户的操作日志、系统日志等，如果每个类都需要自己实现日志记录方法，就会使代码重复率很高。此时，可以使用代理模式，将日志记录的职责交给代理对象来完成，原始对象不需要关心具体的日志记录实现细节。

权限控制
在电商系统中，用户的权限控制十分重要。使用代理模式可以将权限控制的职责交给代理对象来处理，例如，只有管理员才能够删除某些敏感信息或进行系统配置操作，那么可以通过代理对象来判断当前用户是否具有足够的权限。

也可以说项目中没有用到代理，但框架有mybatis，异常管理器，spring


在电商项目中，Spring Cloud Alibaba可以用于以下场景：

商品信息服务：电商项目中商品数量庞大，分散在不同的数据库中，而且每个品类的商品信息也不一样。因此可以使用Dubbo在不同的电商平台之间进行数据的传递，在使用Nacos来进行服务的注册与发现，以达到商品信息的共享。
库存预警服务：在电商场景中，库存数量和销售数据往往有时差，因此可以使用RocketMQ实现异步队列，将库存数据和销售数据进行预警，提前预测到库存不足的商品，避免超卖或者缺货造成的损失。
订单服务：订单服务是电商项目中最重要的服务之一，订单和支付环节更是整个电商系统的核心环节。可以使用RocketMQ和Dubbo实现订单服务的分布式处理，同时使用Sentinel实现限流和熔断保护。
会员服务：电商项目中会员服务也是极为重要的一个环节，会员可以享受到更多的优惠，同时也会拥有更多的权益。可以使用Nacos实现会员服务的注册和发现，使用Dubbo实现会员信息的跨应用调用，从而完成会员服务的全链路保障。

总之，Spring Cloud Alibaba作为一个分布式微服务框架，可以实现服务的注册与发现、熔断降级、分布式配置管理、消息驱动等多种功能，可以为电商项目提供强有力的支持，提高系统的可靠性、可用性和可扩展性。


最新
分布式事务的一致性，是如何实现的

1. 两阶段提交（Two-Phase Commit，2PC）：它是最早的一种分布式事务协议，通过协调者（Coordinator）和参与者（Participant）两个角色来实现分布式事务的一致性。在第一个阶段，协调者向参与者发送可提交事务的通知，等待参与者的响应。如果所有参与者都成功响应，那么协调者发送提交事务的请求，否则协调者发送中止事务的请求。该方法的实现相对简单，但是存在性能和可扩展性不足的问题，且在第二阶段可能有超时问题。
2. 补偿事务（Compensating Transaction）：它通常被称为“回退事务（Rollback Transaction）”或“补偿机制（Compensation Mechanism）”，用于处理在分布式环境下某些节点无法完成事务的情况。其基本思想是，当某个节点无法完成事务时，将执行回滚操作，即撤销之前已经执行的操作。这种方法可以保证一致性，但是比较复杂，需要考虑回滚操作的正确性和数据的一致性。
3. 基于消息的事务（Message-Based Transaction）：在这种方法中，消息系统负责保证事务的一致性。不同的消息队列系统采用的方法不同，但是基本思想都是由消息系统负责管理事务状态和提交事务，从而保证操作的原子性和一致性。该方法的优势在于可以提高系统的可伸缩性和可用性，但需要考虑消息系统的性能和可靠性问题。


在Spring框架中，分布式事务通常使用@Transactional注解来实现。该注解可以应用于方法，也可以应用于类。应用于类时，表示所有公共方法都是事务性的（默认情况下）。
@Transactional注解有以下一些常见的属性：

propagation: 事务的传播行为，默认值为Propagation.REQUIRED。
isolation: 事务的隔离级别，默认值为Isolation.DEFAULT。
timeout: 事务的超时时间，默认值为–1，表示不设置超时时间。
readOnly: 是否只读，默认值为false。
rollbackFor: 触发回滚的异常类型，默认为空数组。
noRollbackFor: 不触发回滚的异常类型，默认为空数组。

电商项目中，Spring Cloud Alibaba可以用于以下场景：
1. 商品信息服务：电商项目中商品数量庞大，分散在不同的数据库中，而且每个品类的商品信息也不一样。因此可以使用Dubbo在不同的电商平台之间进行数据的传递，在使用Nacos来进行服务的注册与发现，以达到商品信息的共享。
具体实现：
1. 在 pom.xml 文件中添加 Nacos 依赖：
2. 在 application.properties 文件中配置 Nacos 注册中心的相关信息：端口号，地址
3. 在启动类上添加 @EnableDiscoveryClient 注解，完成服务注册：
4. 使用 @FeignClient 注解定义服务调用接口，并在接口方法上使用 @PostMapping、@RequestBody 等注解定义具体请求方式和参数：
5. 在其他服务中，使用 @Autowired 或 @Resource 注解注入服务调用接口，并使用接口方法调用服务：
通过以上步骤，我们就可以使用 Nacos 实现服务的注册与发现，并通过服务调用接口进行服务调用。Nacos 还提供了诸如服务健康检查、服务负载均衡、熔断降级等功能，可以满足微服务架构下的多种需求。


2. 订单服务：订单服务是电商项目中最重要的服务之一，订单和支付环节更是整个电商系统的核心环节。可以使用RocketMQ和Dubbo实现订单服务的分布式处理，同时使用Sentinel实现限流和熔断保护。
具体实现：在@RequestMapping 注解中添加 @SentinelResource 来实现限流，熔断保护功能 在注解中设置限流模式、熔断保护和降级处理逻辑  参数有：fallback\blockHandler\exceptionsToTrace


gateway的路由配置参数
在Spring Cloud Gateway中，路由配置提供多种参数设置，可以根据不同的需求进行配置。以下是一些常用的路由配置参数：
1. id：路由的唯一标识符。
2. uri：路由请求转发的目标地址，可以为HTTP、HTTPS或Websocket URI。
3. predicates：用于路由匹配的谓语，例如请求的路径、头部、Cookie等等。常见的谓语有：
  ○ Path：基于路径的匹配谓语，可以配置路由请求匹配队列中的某个路径，例如：path=/my/path/**
  ○ Host：基于主机名的匹配谓语，可以匹配请求的主机名，例如：host=**.example.org
  ○ Method：基于HTTP方法的匹配谓语，可以匹配HTTP请求的方法类型，例如：method=GET
  ○ Query：基于查询参数的匹配谓语，可以匹配请求的查询参数，例如：query=my_param
4. filters：用于对路由请求进行修改或增强的过滤器，例如添加请求头、重定向等等。
5. order：路由的执行顺序。值越小越先执行。同一个路由下，还可以使用多个Filter，order值一般是1234…这种枚举值，用于确定Filter的执行顺序。


如何利用利用redis的setnx命令实现分布式锁，顺便给出java代码例子
Redis 的 setnx 命令可以用来实现分布式锁。setnx 命令是一个原子操作，可以保证同时只有一个客户端能够成功地获得锁。具体的实现步骤如下：
1.当一个客户端需要获取分布式锁时，向 Redis 发送一条带有特定键名（即锁的名称）和值（即客户端标识符）的 setnx 命令。
2.如果键名不存在，则 Redis 会创建该键并将其对应的值设置为客户端标识符，并返回 1，表示获取锁成功。
3.如果键名已经存在，则 Redis 不会进行任何操作，并返回 0，表示获取锁失败。
4.当客户端完成了对共享资源的访问并想要释放锁时，向 Redis 发送一条 del 命令，删除该键。
  public boolean acquire(String lockKey, String requestId, int expireSeconds) {
    // 使用 set 方法在 Redis 中设置键值对，并使用 NX 和 EX 参数来实现分布式锁的特性
    // NX 表示只有当键不存在时才设置值（即只有一个客户端能够
      获取到锁），EX 表示设置键的过期时间（避免锁被永久占用）
        String result = jedis.set(lockKey, requestId, "NX", "EX", expireSeconds);
        return "OK".equals(result);
    }

    public void release(String lockKey, String requestId) {
  // 使用 get 方法获取键对应的值，判断是否与请求标识一致，
        如果一致则使用 del 方法删除该键，释放锁
        if (requestId.equals(jedis.get(lockKey))) {
            jedis.del(lockKey);
        }
    }



订单模块与支付模块之间的分布式事务如何实现

1.引入Seata依赖
2.配置Seata服务器地址
在订单模块和支付模块中，需要配置Seata服务器的地址、端口等信息
3.确认分布式事务边界
确定哪些操作应该放在同一个分布式事务中，这就是确定分布式事务边界的过程。在订单模块中，从下单开始到最终订单状态确定为已支付，整个过程应该放在同一个分布式事务中。在支付模块中，从接收到支付请求开始到最终支付状态确定为成功，整个过程应该放在同一个分布式事务中。
4.在业务代码中添加Seata注解
在订单模块和支付模块中，需要在涉及到分布式事务的方法上添加Seata注解。例如，在订单模块的下单方法中，可以添加如下注解：
@Service
public class OrderServiceImpl implements OrderService {
    @Autowired
    private OrderMapper orderMapper;

    // 下单操作
    @GlobalTransactional   // 添加全局事务注解
    public void placeOrder(Order order) {
        orderMapper.insert(order);
        // 调用支付模块进行支付
        paymentService.pay(order.getId(), order.getAmount());
        // 更新订单状态为已支付
        order.setStatus(OrderStatus.PAID);
        orderMapper.update(order);
    }
}
5.配置数据源代理
由于Seata需要对业务操作进行拦截和代理，因此需要为业务实现的数据源配置代理。在Spring Boot应用中，可以借助Seata提供的DataSourceProxyAutoConfiguration类来实现数据源代理的配置：
6.启动Seata服务器
在以上步骤完成后，就可以启动Seata服务器了。Seata提供了Server、Client和Console三种模式，其中Server模式是必须的，Client和Console模式则是可选的。

定位慢查询SQL语句的步骤如下：
1.开启慢查询日志功能：设置MySQL参数slow_query_log为ON，同时设置long_query_time来标识执行时间超过多少秒的SQL语句被认为是慢查询。


执行explain查询后，最常看的参数是哪些
1.type：表示访问表的方式，是一个很重要的参数。如果是使用了好的索引，type就会显示为const或者ref，否则可能会显示为全表扫描（ALL），这会大大影响查询性能。
2.key：表示实际被使用的索引，如果该值为空，则意味着没有使用合适的索引进行优化查询。
3.rows：表示MySQL估计需要扫描的行数，该值对于查询优化非常重要。如果rows过大，则需要考虑优化查询语句或者增加适当的索引。
4.Extra：表示额外的信息，如Using filesort、Using temporary等。这些信息结合上面的参数一起分析可以帮助我们定位SQL查询的瓶颈。
5.possible_keys：表示所有可能被使用的索引，但不一定全部都会被使用。该参数用于判断是否存在可用的索引，如果该值为空，则说明需要优化查询语句或者添加适当的索引。

项目中遇到的难点
订单模块：事务控制问题：在订单模块中，通常需要对数据库进行增、删、改、查等多种操作，而这些操作需要保证数据的一致性和完整性。因此，在编写订单模块代码时，需要使用事务来控制这些操作的成功或失败，并能够实现回滚操作，确保数据操作的正确性和安全性。

项目中出现的bug
yml配置文件没有对齐导致程序解析文件时出现语法错误